{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4eb1f7b",
   "metadata": {},
   "source": [
    "# Proyecto Dataset Diab√©tico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80167d64",
   "metadata": {},
   "source": [
    "## üìä Resumen del Dataset\n",
    "\n",
    "- **Dimensiones**: 101,766 registros √ó 50 columnas\n",
    "- **Variable objetivo**: `readmitted` (NO, <30 d√≠as, >30 d√≠as)\n",
    "- **Tipo de problema**: Clasificaci√≥n multiclase (puede convertirse a binaria)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1946570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "from sklearn.impute import SimpleImputer\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff81b9b",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# LINDA\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6554489f",
   "metadata": {},
   "source": [
    "### Introducci√≥n y carga de datos\n",
    "_Aqu√≠ daremos una breve descripci√≥n del problema, marcaremos ,los objetivos del proyecto y se har√° la carga del dataset y la inspecci√≥n inicial._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bb9781",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/diabetic_data.csv\")\n",
    "    \n",
    "print(f\"\\n‚úì Dataset cargado exitosamente\")\n",
    "print(f\"  Dimensiones originales: {df.shape[0]} filas √ó {df.shape[1]} columnas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28629d5a",
   "metadata": {},
   "source": [
    "### Inspecci√≥n Inicial del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2194b452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informaci√≥n general del dataset\n",
    "print(\"=\" * 80)\n",
    "print(\"INFORMACI√ìN GENERAL DEL DATASET\")\n",
    "print(\"=\" * 80)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87148f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeras filas del dataset\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PRIMERAS 5 FILAS DEL DATASET\")\n",
    "print(\"=\" * 80)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65bdcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estad√≠sticas descriptivas\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ESTAD√çSTICAS DESCRIPTIVAS\")\n",
    "print(\"=\" * 80)\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3907409",
   "metadata": {},
   "source": [
    "## Limpieza y Preprocesamiento de Datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11df2aff",
   "metadata": {},
   "source": [
    "### An√°lisis de valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10a8009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de valores faltantes\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"AN√ÅLISIS DE VALORES FALTANTES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Valores '?' que se consideran como faltantes\n",
    "missing_counts = (df == '?').sum()\n",
    "missing_counts = missing_counts[missing_counts > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_counts) > 0:\n",
    "    print(\"\\nColumnas con '?' (valores faltantes):\")\n",
    "    for col, count in missing_counts.items():\n",
    "        pct = (count / len(df)) * 100\n",
    "        print(f\"  {col:30s}: {count:6d} ({pct:5.2f}%)\")\n",
    "else:\n",
    "    print(\"No hay valores '?' en el dataset\")\n",
    "\n",
    "# Valores NaN tradicionales\n",
    "nan_counts = df.isnull().sum()\n",
    "nan_counts = nan_counts[nan_counts > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(nan_counts) > 0:\n",
    "    print(\"\\nColumnas con NaN:\")\n",
    "    for col, count in nan_counts.items():\n",
    "        pct = (count / len(df)) * 100\n",
    "        print(f\"  {col:30s}: {count:6d} ({pct:5.2f}%)\")\n",
    "else:\n",
    "    print(\"\\nNo hay valores NaN en el dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575de0d0",
   "metadata": {},
   "source": [
    "### An√°lisis de duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf311421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis de duplicados\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"AN√ÅLISIS DE DUPLICADOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Duplicados por encounter_id (cada encuentro debe ser √∫nico)\n",
    "duplicate_encounters = df['encounter_id'].duplicated().sum()\n",
    "print(f\"Registros duplicados por encounter_id: {duplicate_encounters}\")\n",
    "\n",
    "# Duplicados por patient_nbr (un paciente puede tener m√∫ltiples encuentros)\n",
    "unique_patients = df['patient_nbr'].nunique()\n",
    "total_encounters = len(df)\n",
    "print(f\"Pacientes √∫nicos: {unique_patients}\")\n",
    "print(f\"Total de encuentros: {total_encounters}\")\n",
    "print(f\"Promedio de encuentros por paciente: {total_encounters / unique_patients:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139cadeb",
   "metadata": {},
   "source": [
    "### Eliminaci√≥n de columnas irrelevantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212c1c34",
   "metadata": {},
   "source": [
    "**Eliminaci√≥n de columnas con mayor√≠a de nulos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e539148",
   "metadata": {},
   "source": [
    "Dado que la columna ``weight`` tiene un 97% de nulos, consideramos que la mejor estrategia es eliminarla. As√≠mismo, con un p-value de ``0.83``, las diferencias en tasas de readmisi√≥n entre las categor√≠as de HbA1c (``A1CResult``) NO son estad√≠sticamente significativas.\n",
    "Esto significa que la tasa de readmisi√≥n NO var√≠a significativamente seg√∫n el control gluc√©mico medido por HbA1c\n",
    "HbA1c NO tiene poder predictivo para readmisi√≥n <30 d√≠as en este dataset. Cualquier diferencia observada es atribuible al azar. Por eso la decisi√≥n de eliminarla directamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f0767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columna 'weight'\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"AN√ÅLISIS Y ELIMINACI√ìN DE 'WEIGHT'\")\n",
    "print(\"=\" * 80)\n",
    "df = df.drop(columns=['weight', 'A1Cresult'])\n",
    "print(f\"  Dimensiones despu√©s de eliminar 'weight' y 'A1Cresult': {df.shape[0]} filas √ó {df.shape[1]} columnas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dc7797",
   "metadata": {},
   "source": [
    "**Eliminaci√≥n de IDs no informativos: ``encounter_id`` y ``patient_nbr``** <br><br>\n",
    "Eliminamos ``encounter_id`` y ``patient_nbr`` porque son IDENTIFICADORES no informativos:\n",
    "\n",
    "1.  **encounter_id (ID de encuentro hospitalario)**: Es un identificador √öNICO para cada registro (101,766 valores √∫nicos) que no tiene valor predictivo: es simplemente un n√∫mero de referencia administrativo.\n",
    "\n",
    "\n",
    "2.  **patient_nbr (ID de paciente)**: Aunque un paciente puede tener m√∫ltiples ingresos (promedio: 1.42 encuentros), el ID del paciente en s√≠ mismo NO es predictivo. Mantener 'patient_nbr' introducir√≠a sesgo hacia pacientes espec√≠ficos en lugar de caracter√≠sticas cl√≠nicas generalizables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474dc6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ELIMINACI√ìN DE IDs NO INFORMATIVOS: encounter_id y patient_nbr\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Verificar unicidad de encounter_id\n",
    "print(f\"\\nencounter_id:\")\n",
    "print(f\"  Valores √∫nicos: {df['encounter_id'].nunique():,}\")\n",
    "print(f\"  Total de filas: {len(df):,}\")\n",
    "print(f\"  ¬øTodos √∫nicos?: {df['encounter_id'].nunique() == len(df)}\")\n",
    "\n",
    "# Analizar patient_nbr\n",
    "print(f\"\\npatient_nbr:\")\n",
    "print(f\"  Pacientes √∫nicos: {df['patient_nbr'].nunique():,}\")\n",
    "print(f\"  Total de encuentros: {len(df):,}\")\n",
    "print(f\"  Promedio de encuentros por paciente: {len(df) / df['patient_nbr'].nunique():.2f}\")\n",
    "\n",
    "# Proceder con la eliminaci√≥n\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ELIMINANDO COLUMNAS ID...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df = df.drop(columns=['encounter_id', 'patient_nbr'])\n",
    "\n",
    "print(f\"‚úì Columnas 'encounter_id' y 'patient_nbr' eliminadas exitosamente\")\n",
    "print(f\"‚úì Dimensiones actuales del dataset: {df.shape[0]:,} filas √ó {df.shape[1]} columnas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e18c8e",
   "metadata": {},
   "source": [
    "### Imputaci√≥n de valores faltantes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981ed111",
   "metadata": {},
   "source": [
    "**Imputaci√≥n de ``race`` con la moda** <br><br>\n",
    "La variable ``race`` presenta solo un 2.23% de valores faltantes, una proporci√≥n relativamente baja que no compromete significativamente el dataset.\n",
    "\n",
    "Decidimos imputar con la MODA por las siguientes razones:\n",
    "\n",
    "1. **Preservaci√≥n de datos**: Eliminar estos registros reducir√≠a innecesariamente\n",
    "   el tama√±o del dataset (~2,300 registros).\n",
    "\n",
    "2. **M√≠nimo sesgo introducido**: Con solo 2.23% de valores faltantes, la imputaci√≥n\n",
    "   con moda introduce un sesgo m√≠nimo en la distribuci√≥n de la variable.\n",
    "\n",
    "3. **Alternativas descartadas**:\n",
    "   - Crear categor√≠a \"Unknown\": No aporta informaci√≥n y puede confundir a los modelos\n",
    "   - Imputaci√≥n predictiva: Complejidad injustificada para un 2.23% de faltantes\n",
    "\n",
    "La imputaci√≥n con moda es una estrategia conservadora y apropiada para variables\n",
    "categ√≥ricas con baja proporci√≥n de valores faltantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981ed111",
   "metadata": {},
   "source": [
    "**Imputaci√≥n de ``race`` con la moda**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec560d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"IMPUTACI√ìN DE 'RACE' CON LA MODA\")\n",
    "print(\"=\" * 80)\n",
    "# Paso 1: Convertir '?' a NaN solo en la columna 'race'\n",
    "df['race'] = df['race'].replace('?', np.nan)\n",
    "\n",
    "# Paso 2: Backup y verificaci√≥n\n",
    "print(\"Valores nulos originales en 'race':\", df['race'].isna().sum())\n",
    "df['race_backup'] = df['race'].copy()\n",
    "\n",
    "# Paso 3: Imputaci√≥n\n",
    "race_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df['race'] = race_imputer.fit_transform(df[['race']]).ravel()\n",
    "\n",
    "# Paso 4: Identificar valores imputados\n",
    "imputed_mask = df['race_backup'].isna() & df['race'].notna()\n",
    "imputed_values = df.loc[imputed_mask, 'race']\n",
    "\n",
    "print(f\"\\nSe imputaron {len(imputed_values)} valores:\")\n",
    "print(imputed_values.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d28014",
   "metadata": {},
   "source": [
    "### Sustituci√≥n de Valores Faltantes en `payer_code` y `medical_specialty`\n",
    "\n",
    "Optamos por crear una categor√≠a expl√≠cita `\"Unknown\"` en lugar de eliminar registros o imputar con la moda, ya que la ausencia de informaci√≥n en estas columnas puede ser informativa por s√≠ misma.\n",
    "\n",
    "La falta de informaci√≥n del pagador (`payer_code`) puede reflejar situaciones como pacientes sin seguro m√©dico o casos de emergencia donde la documentaci√≥n no fue completada adecuadamente. De manera similar, la ausencia de especialidad m√©dica asignada (`medical_specialty`) puede indicar atenci√≥n generalista o admisiones de emergencia sin derivaci√≥n espec√≠fica.\n",
    "\n",
    "Ambas columnas presentan una proporci√≥n considerable de valores faltantes. Eliminar estos registros comprometer√≠a significativamente la capacidad de an√°lisis, mientras que la imputaci√≥n con moda introducir√≠a sesgo al asumir que todos los valores faltantes pertenecen a la categor√≠a m√°s frecuente.\n",
    "\n",
    "Al preservar estos registros mediante la categor√≠a `\"Unknown\"`, mantenemos la integridad del dataset y permitimos que los modelos de machine learning eval√∫en si la ausencia de esta informaci√≥n administrativa es predictiva de readmisi√≥n hospitalaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6223bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"AN√ÅLISIS DE VALORES FALTANTES: payer_code y medical_specialty\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calcular porcentajes de valores '?' (que ser√°n reemplazados por 'Unknown')\n",
    "total_rows = len(df)\n",
    "\n",
    "payer_missing = (df['payer_code'] == '?').sum()\n",
    "payer_pct = (payer_missing / total_rows) * 100\n",
    "\n",
    "specialty_missing = (df['medical_specialty'] == '?').sum()\n",
    "specialty_pct = (specialty_missing / total_rows) * 100\n",
    "\n",
    "print(f\"\\npayer_code:\")\n",
    "print(f\"  Valores '?': {payer_missing:,} ({payer_pct:.2f}%)\")\n",
    "\n",
    "print(f\"\\nmedical_specialty:\")\n",
    "print(f\"  Valores '?': {specialty_missing:,} ({specialty_pct:.2f}%)\")\n",
    "\n",
    "print(f\"\\n‚úì Estos valores ser√°n reemplazados por 'Unknown'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7348fe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sustituir '?' por 'Unknown' en payer_code y medical_specialty\n",
    "df['payer_code'] = df['payer_code'].replace('?', 'Unknown')\n",
    "df['medical_specialty'] = df['medical_specialty'].replace('?', 'Unknown')\n",
    "\n",
    "print(f\"‚úì Valores '?' reemplazados por 'Unknown' en payer_code y medical_specialty\")\n",
    "print(f\"  payer_code: {(df['payer_code'] == 'Unknown').sum()} valores 'Unknown'\")\n",
    "print(f\"  medical_specialty: {(df['medical_specialty'] == 'Unknown').sum()} valores 'Unknown'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137603a9",
   "metadata": {},
   "source": [
    "### Transformaci√≥n de Variable Objetivo\n",
    "El dataset original contiene la variable `readmitted` con tres categor√≠as:\n",
    "- `NO`: El paciente no fue readmitido\n",
    "- `>30`: El paciente fue readmitido despu√©s de 30 d√≠as\n",
    "- `<30`: El paciente fue readmitido en menos de 30 d√≠as\n",
    "\n",
    "Nuestro objetivo es **predecir readmisiones tempranas** (en menos de 30 d√≠as), ya que estas representan:\n",
    "\n",
    "1. **Mayor costo para el sistema de salud:** Las readmisiones tempranas suelen estar relacionadas con complicaciones evitables o alta prematura. En EEUU, donde se recopil√≥ la informaci√≥n, Medicare suele penalizar a los hospitales cuyos pacientes son ingresados de nuevo en un per√≠odo de 30 d√≠as o menos.\n",
    "\n",
    "2. **Indicador de calidad asistencial:** Una readmisi√≥n en <30 d√≠as puede se√±alar deficiencias en el plan de alta, seguimiento inadecuado o falta de educaci√≥n al paciente.\n",
    "\n",
    "3. **Oportunidad de intervenci√≥n:** Identificar pacientes en riesgo de readmisi√≥n temprana permite implementar programas de seguimiento intensivo post-alta.\n",
    "<br><br>\n",
    "\n",
    "\n",
    "**Decisi√≥n de Modelado** <br>\n",
    "Transformaremos el problema en **clasificaci√≥n binaria**:\n",
    "- **Clase positiva (1):** Readmisi√≥n en <30 d√≠as\n",
    "- **Clase negativa (0):** No readmisi√≥n o readmisi√≥n >30 d√≠as\n",
    "\n",
    "Esta decisi√≥n implica que tratamos las readmisiones tard√≠as (>30 d√≠as) como \"no problem√°ticas\" desde la perspectiva de calidad asistencial inmediata, enfoc√°ndonos en prevenir las readmisiones que ocurren poco despu√©s del alta hospitalaria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb370a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TRANSFORMACI√ìN DE VARIABLE OBJETIVO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# PASO 1: Verificar si existe la columna original 'readmitted'\n",
    "# PASO 1: Verificar existencia y guardar copia de la columna 'readmitted'\n",
    "if 'readmitted' not in df.columns:\n",
    "    raise ValueError(\"Columna 'readmitted' no encontrada. Carga el dataset original (diabetic_data.csv)\")\n",
    "\n",
    "print(\"\\n‚úì Columna 'readmitted' encontrada\")\n",
    "print(\"\\nDistribuci√≥n original:\")\n",
    "for cat, cnt in df['readmitted'].value_counts().items():\n",
    "    print(f\"  {cat:>3}: {cnt:>6,} ({cnt/len(df)*100:>5.2f}%)\")\n",
    "\n",
    "original_readmitted = df['readmitted'].copy()\n",
    "\n",
    "# PASO 2: Crear/Sobrescribir readmitted_binary CORRECTAMENTE\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CREANDO VARIABLE BINARIA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Transformaci√≥n CORRECTA: '<30' ‚Üí 1, resto (NO + >30) ‚Üí 0\n",
    "df['readmitted_binary'] = (df['readmitted'] == '<30').astype(int)\n",
    "\n",
    "print(\"\\n‚úì Transformaci√≥n aplicada:\")\n",
    "print(\"   L√≥gica: '<30' ‚Üí 1 (Readmitido <30 d√≠as)\")\n",
    "print(\"          'NO' + '>30' ‚Üí 0 (No readmitido <30 d√≠as)\")\n",
    "\n",
    "# PASO 3: Verificaci√≥n cruzada\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VERIFICACI√ìN CRUZADA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "cross_check = pd.crosstab(df['readmitted'], df['readmitted_binary'], margins=True)\n",
    "print(\"\\nTabla de contingencia (readmitted vs readmitted_binary):\")\n",
    "print(cross_check)\n",
    "\n",
    "# PASO 4: Distribuci√≥n final\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DISTRIBUCI√ìN FINAL DE VARIABLE OBJETIVO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "readmit_counts = df['readmitted_binary'].value_counts().sort_index()\n",
    "cnt0 = int(readmit_counts.get(0, 0))\n",
    "cnt1 = int(readmit_counts.get(1, 0))\n",
    "\n",
    "print(\"\\nDistribuci√≥n de 'readmitted_binary':\")\n",
    "print(f\"  0 (Sin readmisi√≥n <30 d√≠as): {cnt0:,} ({(cnt0/len(df)*100):.2f}%)\")\n",
    "print(f\"  1 (Readmitido <30 d√≠as):      {cnt1:,} ({(cnt1/len(df)*100):.2f}%)\")\n",
    "\n",
    "# Calcular desbalanceo\n",
    "if cnt1 > 0:\n",
    "    imbalance_ratio = cnt0 / cnt1\n",
    "    print(f\"\\n  Ratio de desbalanceo: {imbalance_ratio:.2f}:1\")\n",
    "    \n",
    "    if imbalance_ratio > 3:\n",
    "        print(f\"    ‚Üí Dataset DESBALANCEADO. Se recomienda:\")\n",
    "        print(f\"       ‚Ä¢ SMOTE para oversampling de la clase minoritaria\")\n",
    "        print(f\"       ‚Ä¢ class_weight='balanced' en los modelos\")\n",
    "        print(f\"       ‚Ä¢ Priorizar m√©tricas: Recall, F1-Score, AUC-ROC\")\n",
    "    else:\n",
    "        print(f\"    ‚Üí Desbalanceo moderado. Monitorear F1-Score y AUC-ROC\")\n",
    "else:\n",
    "    print(\"\\n  ‚ö†Ô∏è  ERROR: No hay casos positivos (clase 1)\")\n",
    "    raise ValueError(\"La transformaci√≥n fall√≥: no hay casos de readmisi√≥n <30 d√≠as\")\n",
    "\n",
    "# PASO 5: Validaci√≥n final\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VALIDACI√ìN FINAL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Valores esperados seg√∫n el dataset original\n",
    "expected_0 = (df['readmitted'].isin(['NO', '>30'])).sum()\n",
    "expected_1 = (df['readmitted'] == '<30').sum()\n",
    "\n",
    "print(f\"\\nValores esperados:\")\n",
    "print(f\"  Clase 0: {expected_0:,}\")\n",
    "print(f\"  Clase 1: {expected_1:,}\")\n",
    "\n",
    "print(f\"\\nValores obtenidos:\")\n",
    "print(f\"  Clase 0: {cnt0:,}\")\n",
    "print(f\"  Clase 1: {cnt1:,}\")\n",
    "\n",
    "# Verificar coincidencia\n",
    "if cnt0 == expected_0 and cnt1 == expected_1:\n",
    "    print(\"\\n‚úÖ VALIDACI√ìN EXITOSA: La transformaci√≥n es CORRECTA\")\n",
    "else:\n",
    "    print(\"\\n‚ùå ERROR EN LA VALIDACI√ìN\")\n",
    "    print(f\"   Diferencia clase 0: {cnt0 - expected_0}\")\n",
    "    print(f\"   Diferencia clase 1: {cnt1 - expected_1}\")\n",
    "    raise ValueError(\"La transformaci√≥n no coincide con los valores esperados\")\n",
    "\n",
    "# PASO 6: Eliminar columna original para evitar data leakage\n",
    "df = df.drop(columns=['readmitted'], errors='ignore')\n",
    "print(f\"\\n‚úì Columna 'readmitted' eliminada para evitar data leakage\")\n",
    "print(f\"‚úì Variable objetivo final: 'readmitted_binary'\")\n",
    "\n",
    "# PASO 7: Visualizaciones\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERANDO VISUALIZACIONES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Gr√°fico 1: Distribuci√≥n original (3 categor√≠as)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax = axes[0]\n",
    "sns.countplot(x=original_readmitted, order=original_readmitted.value_counts().index, ax=ax, palette='Set2')\n",
    "ax.set_title('Distribuci√≥n de Variable Objetivo Original (3 categor√≠as)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('readmitted', fontsize=12)\n",
    "ax.set_ylabel('Cantidad de Pacientes', fontsize=12)\n",
    "\n",
    "total_orig = len(original_readmitted)\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.annotate(f'{int(height):,}\\n({(height/total_orig)*100:.2f}%)',\n",
    "                (p.get_x() + p.get_width() / 2, height),\n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Gr√°fico 2: Distribuci√≥n binaria (2 categor√≠as)\n",
    "ax = axes[1]\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "labels = ['0 (No Readmitido <30)', '1 (Readmitido <30)']\n",
    "\n",
    "bars = ax.bar(labels, readmit_counts.values, color=colors, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "for bar, count in zip(bars, readmit_counts.values):\n",
    "    height = bar.get_height()\n",
    "    pct = (count / len(df)) * 100\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{count:,}\\n({pct:.2f}%)',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_title('Distribuci√≥n de Variable Objetivo Binaria', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Cantidad de Pacientes', fontsize=12)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# L√≠nea de desbalanceo\n",
    "if cnt1 > 0:\n",
    "    ax.axhline(y=cnt1, color='red', linestyle='--', linewidth=2, label=f'L√≠nea de Desbalanceo (Clase 1)')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ TRANSFORMACI√ìN COMPLETADA EXITOSAMENTE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036c983d",
   "metadata": {},
   "source": [
    "## Encoding de Variables Categ√≥ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86713c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar columnas categ√≥ricas y num√©ricas\n",
    "print(\"=\" * 80)\n",
    "print(\"PREPARACI√ìN PARA ENCODING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Columnas a excluir del encoding (IDs y columnas ya procesadas)\n",
    "exclude_cols = ['encounter_id', 'patient_nbr', 'readmitted', 'readmitted_binary', 'early_readmission']\n",
    "\n",
    "# Identificar columnas categ√≥ricas (object type)\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "categorical_cols = [col for col in categorical_cols if col not in exclude_cols]\n",
    "\n",
    "print(f\"\\nColumnas categ√≥ricas identificadas ({len(categorical_cols)}):\")\n",
    "for col in categorical_cols:\n",
    "    unique_count = df[col].nunique()\n",
    "    print(f\"  {col:30s}: {unique_count:3d} valores √∫nicos\")\n",
    "\n",
    "# Separar entre nominales (muchos valores) y binarias/ordinales\n",
    "high_cardinality_cols = [col for col in categorical_cols if df[col].nunique() > 10]\n",
    "low_cardinality_cols = [col for col in categorical_cols if df[col].nunique() <= 10]\n",
    "\n",
    "print(f\"\\nColumnas de alta cardinalidad (>10 valores): {len(high_cardinality_cols)}\")\n",
    "print(f\"Columnas de baja cardinalidad (‚â§10 valores): {len(low_cardinality_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07d4e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar Label Encoding a columnas de baja cardinalidad\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"LABEL ENCODING (para columnas de baja cardinalidad)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "label_encoders = {}\n",
    "df_encoded = df.copy()\n",
    "\n",
    "for col in low_cardinality_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    print(f\"‚úì {col}: {len(le.classes_)} clases codificadas\")\n",
    "\n",
    "print(f\"\\n‚úì Total de columnas con Label Encoding: {len(low_cardinality_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3e7a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar One-Hot Encoding a columnas de alta cardinalidad\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ONE-HOT ENCODING (para columnas de alta cardinalidad)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Para evitar explosi√≥n dimensional, limitaremos las columnas de alta cardinalidad\n",
    "# o usaremos frequency encoding como alternativa\n",
    "\n",
    "print(f\"\\nColumnas de alta cardinalidad que se procesar√°n:\")\n",
    "for col in high_cardinality_cols:\n",
    "    print(f\"  {col}: {df_encoded[col].nunique()} valores √∫nicos\")\n",
    "\n",
    "# Decisi√≥n: Para payer_code (18 valores) aplicaremos One-Hot\n",
    "# Para las dem√°s columnas con demasiados valores (diagnosis codes),\n",
    "# usaremos Label Encoding ya que One-Hot crear√≠a demasiadas columnas\n",
    "\n",
    "# Columnas para One-Hot (cardinalidad moderada)\n",
    "onehot_cols = ['payer_code']  # 18 valores √∫nicos es manejable\n",
    "onehot_cols = [col for col in onehot_cols if col in high_cardinality_cols]\n",
    "\n",
    "if onehot_cols:\n",
    "    df_encoded = pd.get_dummies(df_encoded, columns=onehot_cols, prefix=onehot_cols, drop_first=True)\n",
    "    print(f\"\\n‚úì One-Hot Encoding aplicado a: {onehot_cols}\")\n",
    "    print(f\"  Nuevas columnas creadas: {len([c for c in df_encoded.columns if any(pc in c for pc in onehot_cols)])}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No hay columnas seleccionadas para One-Hot Encoding\")\n",
    "\n",
    "# Para las dem√°s columnas de alta cardinalidad, aplicar Label Encoding\n",
    "remaining_high_card = [col for col in high_cardinality_cols if col not in onehot_cols]\n",
    "for col in remaining_high_card:\n",
    "    le = LabelEncoder()\n",
    "    df_encoded[col] = le.fit_transform(df_encoded[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    print(f\"‚úì Label Encoding aplicado a {col}\")\n",
    "\n",
    "print(f\"\\n‚úì Encoding completo. Nueva dimensi√≥n: {df_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58dbfdd",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "En esta secci√≥n creamos tres features clave:\n",
    "\n",
    "**1. `total_visits`**: Historial total de visitas hospitalarias. <br>\n",
    "Suma de visitas ambulatorias (`number_outpatient`), emergencias (`number_emergency`) e ingresos previos (`number_inpatient`).\n",
    "\n",
    "Los pacientes con historial extenso de hospitalizaciones pueden tener condiciones cr√≥nicas m√°s complejas o menor adherencia al tratamiento, incrementando el riesgo de readmisi√≥n temprana.\n",
    "\n",
    "---\n",
    "\n",
    "**2. `medication_changes`**: Indicador de cambios en tratamiento\n",
    "Suma de `change` (cambio en medicaci√≥n durante hospitalizaci√≥n) y `diabetesMed` (prescripci√≥n de medicamentos para diabetes).\n",
    "\n",
    "Los ajustes de medicaci√≥n reflejan inestabilidad cl√≠nica o complejidad en el manejo de la diabetes. Pacientes que requieren cambios frecuentes pueden tener mayor dificultad para mantener control gluc√©mico post-alta.\n",
    "\n",
    "---\n",
    "\n",
    "**3. `procedures_per_day`**: Intensidad de la atenci√≥n hospitalaria\n",
    "Ratio entre el n√∫mero de procedimientos realizados (`num_procedures`) y los d√≠as de hospitalizaci√≥n (`time_in_hospital`).\n",
    "\n",
    "Una alta densidad de procedimientos puede indicar casos m√°s cr√≠ticos o complicaciones durante la estancia, factores asociados con mayor probabilidad de readmisi√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "Estas variables condensan informaci√≥n cl√≠nica relevante que puede no ser evidente en las variables individuales, facilitando que los modelos identifiquen patrones de riesgo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae73289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Engineering\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Feature 1: Total de visitas (outpatient + emergency + inpatient)\n",
    "df_encoded['total_visits'] = (df_encoded['number_outpatient'] + \n",
    "                               df_encoded['number_emergency'] + \n",
    "                               df_encoded['number_inpatient'])\n",
    "print(f\"‚úì Feature creada: total_visits\")\n",
    "print(f\"  Rango: [{df_encoded['total_visits'].min()}, {df_encoded['total_visits'].max()}]\")\n",
    "print(f\"  Media: {df_encoded['total_visits'].mean():.2f}\")\n",
    "\n",
    "# Feature 2: Cambios en medicaci√≥n (change + diabetesMed)\n",
    "# Nota: 'change' y 'diabetesMed' ya fueron codificadas con LabelEncoder\n",
    "# en la secci√≥n anterior, por lo que ya son variables num√©ricas\n",
    "df_encoded['medication_changes'] = df_encoded['change'] + df_encoded['diabetesMed']\n",
    "print(f\"\\n‚úì Feature creada: medication_changes\")\n",
    "print(f\"  Rango: [{df_encoded['medication_changes'].min()}, {df_encoded['medication_changes'].max()}]\")\n",
    "print(f\"  Distribuci√≥n:\")\n",
    "print(df_encoded['medication_changes'].value_counts().sort_index())\n",
    "\n",
    "# Feature 3: Ratio de procedimientos por d√≠a hospitalizado\n",
    "df_encoded['procedures_per_day'] = df_encoded['num_procedures'] / (df_encoded['time_in_hospital'] + 1)  # +1 para evitar divisi√≥n por 0\n",
    "print(f\"\\n‚úì Feature creada: procedures_per_day\")\n",
    "print(f\"  Media: {df_encoded['procedures_per_day'].mean():.2f}\")\n",
    "\n",
    "print(f\"\\n‚úì Feature Engineering completado. Total de features: {df_encoded.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4955e7c2",
   "metadata": {},
   "source": [
    "### Categorizaci√≥n ICD-9\n",
    "El ICD-9 (_International Classification of Diseases, 9¬™ revisi√≥n_) es un sistema estandarizado utilizado para **clasificar y codificar enfermedades, s√≠ntomas, causas externas y ciertos procedimientos m√©dicos**. Cada diagn√≥stico se representa mediante un c√≥digo num√©rico de 3 a 5 d√≠gitos:\n",
    "- **Primeros 3 d√≠gitos**: definen la categor√≠a general (tipo de enfermedad o condici√≥n).\n",
    "- **D√≠gitos adicionales**: aportan mayor especificidad, indicando detalles como localizaci√≥n, tipo, gravedad u otras caracter√≠sticas.\n",
    "\n",
    "La estructura del ICD-9 se organiza en **cap√≠tulos tem√°ticos**, agrupando las patolog√≠as seg√∫n el sistema afectado (por ejemplo: _enfermedades infecciosas, sistema respiratorio, sistema cardiovascular_, entre otros).\n",
    "\n",
    "En esta secci√≥n nuestro objetivo es transformar **c√≥digos espec√≠ficos** (250.01 o 428.0, por ejemplo) en **etiquetas interpretables** como _Diabetes, Circulatory, Respiratory_, entre otros. Esto facilita el modelado y an√°lisis estad√≠stico, ya que:\n",
    "\n",
    "- Reduce la complejidad del an√°lisis\n",
    "- Mantiene el equilibrio entre granularidad y simplicidad, capturando informaci√≥n cl√≠nica relevante.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7df12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_diagnosis_detailed(diag_code):\n",
    "    \"\"\"\n",
    "    Agrupa c√≥digos ICD-9 en categor√≠as principales seg√∫n cap√≠tulos est√°ndar.\n",
    "    Pensada para c√≥digos de diagn√≥stico (no de procedimiento).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        diag = str(diag_code).strip().upper()\n",
    "        if not diag or diag == '?' or diag == 'NAN':\n",
    "            return 'Unknown'\n",
    "        \n",
    "        if diag.startswith('E'):\n",
    "            return 'External_Causes'\n",
    "        if diag.startswith('V'):\n",
    "            return 'Supplementary'\n",
    "        \n",
    "        if '.' in diag:\n",
    "            code_main = int(diag.split('.')[0])\n",
    "        else:\n",
    "            code_main = int(float(diag))\n",
    "        \n",
    "        # ===== SUBCATEGOR√çAS ESPEC√çFICAS =====\n",
    "        # Diabetes y complicaciones\n",
    "        if code_main == 250:\n",
    "            return 'Diabetes'\n",
    "        \n",
    "        # Enfermedades cardiovasculares (alta correlaci√≥n con readmisi√≥n)\n",
    "        if code_main in range(410, 415):  # 410-414: Enfermedad isqu√©mica del coraz√≥n\n",
    "            return 'Circulatory_Ischemic_Heart'\n",
    "        if code_main in range(428, 429):  # 428: Insuficiencia card√≠aca\n",
    "            return 'Circulatory_Heart_Failure'\n",
    "        if code_main in range(390, 460):  # Resto de circulatorias\n",
    "            return 'Circulatory_Other'\n",
    "        \n",
    "        # Enfermedades respiratorias (com√∫n en hospitalizaciones)\n",
    "        if code_main in range(480, 488):  # Neumon√≠a\n",
    "            return 'Respiratory_Pneumonia'\n",
    "        if code_main in range(490, 493):  # EPOC\n",
    "            return 'Respiratory_COPD'\n",
    "        if code_main in range(460, 520):  # Resto\n",
    "            return 'Respiratory_Other'\n",
    "        \n",
    "        # Enfermedades renales (complicaci√≥n diab√©tica)\n",
    "        if code_main in range(580, 590):  # Nefritis, nefrosis\n",
    "            return 'Genitourinary_Renal'\n",
    "        if code_main in range(590, 630):\n",
    "            return 'Genitourinary_Other'\n",
    "        \n",
    "        # ===== CAP√çTULOS GENERALES =====\n",
    "        if 1 <= code_main < 140:\n",
    "            return 'Infectious_Parasitic'\n",
    "        if 140 <= code_main < 240:\n",
    "            return 'Neoplasms'\n",
    "        if 240 <= code_main < 280:\n",
    "            return 'Endocrine_Metabolic'\n",
    "        if 280 <= code_main < 290:\n",
    "            return 'Blood_Diseases'\n",
    "        if 290 <= code_main < 320:\n",
    "            return 'Mental_Disorders'\n",
    "        if 320 <= code_main < 390:\n",
    "            return 'Nervous_Sense'\n",
    "        if 520 <= code_main < 580:\n",
    "            return 'Digestive'\n",
    "        if 630 <= code_main < 680:\n",
    "            return 'Pregnancy_Childbirth'\n",
    "        if 680 <= code_main < 710:\n",
    "            return 'Skin_Subcutaneous'\n",
    "        if 710 <= code_main < 740:\n",
    "            return 'Musculoskeletal'\n",
    "        if 740 <= code_main < 760:\n",
    "            return 'Congenital_Anomalies'\n",
    "        if 760 <= code_main < 780:\n",
    "            return 'Perinatal_Conditions'\n",
    "        if 780 <= code_main < 800:\n",
    "            return 'Symptoms_Signs'\n",
    "        if 800 <= code_main < 1000:\n",
    "            return 'Injury_Poisoning'\n",
    "        \n",
    "        return 'Other'\n",
    "    \n",
    "    except (ValueError, AttributeError, IndexError):\n",
    "        return 'Unknown'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aaac5e",
   "metadata": {},
   "source": [
    "**Nota:** El balanceo de clases se manejar√° durante el entrenamiento de modelos usando `class_weight='balanced'` en los algoritmos o aplicando SMOTE si es necesario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588d9521",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CATEGORIZACI√ìN DE C√ìDIGOS ICD-9\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Aplicar categorizaci√≥n a los 3 diagn√≥sticos\n",
    "for diag_col in ['diag_1', 'diag_2', 'diag_3']:\n",
    "    if diag_col in df_encoded.columns:\n",
    "        # Crear columna categorizada\n",
    "        cat_col = f\"{diag_col}_category\"\n",
    "        df_encoded[cat_col] = df_encoded[diag_col].apply(categorize_diagnosis_detailed)\n",
    "        \n",
    "        print(f\"‚úì {diag_col} categorizado ‚Üí {cat_col}\")\n",
    "        print(f\"  Categor√≠as encontradas: {df_encoded[cat_col].nunique()}\")\n",
    "        print(f\"  Top 5 categor√≠as:\")\n",
    "        print(df_encoded[cat_col].value_counts().head())\n",
    "        print()\n",
    "\n",
    "# Despu√©s de categorizar, ELIMINAR las columnas originales diag_1, diag_2, diag_3\n",
    "# (ya que ahora tenemos las categor√≠as)\n",
    "df_encoded = df_encoded.drop(columns=['diag_1', 'diag_2', 'diag_3'])\n",
    "print(\"‚úì Columnas originales diag_1, diag_2, diag_3 eliminadas\")\n",
    "\n",
    "# Ahora aplicar Label Encoding a las nuevas columnas categorizadas\n",
    "for cat_col in ['diag_1_category', 'diag_2_category', 'diag_3_category']:\n",
    "    if cat_col in df_encoded.columns:\n",
    "        le = LabelEncoder()\n",
    "        df_encoded[cat_col] = le.fit_transform(df_encoded[cat_col])\n",
    "        print(f\"‚úì Label Encoding aplicado a {cat_col}\")\n",
    "\n",
    "print(f\"\\n‚úì Categorizaci√≥n ICD-9 completada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165999ac",
   "metadata": {},
   "source": [
    "## Normalizaci√≥n de variables num√©ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d615263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Standard Scaler para variables num√©ricas\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"NORMALIZACI√ìN DE VARIABLES NUM√âRICAS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Identificar columnas num√©ricas (excluyendo las variables objetivo y IDs)\n",
    "numerical_cols = ['time_in_hospital', 'num_lab_procedures', 'num_procedures', \n",
    "                  'num_medications', 'number_outpatient', 'number_emergency', \n",
    "                  'number_inpatient', 'number_diagnoses', 'total_visits', \n",
    "                  'medication_changes', 'procedures_per_day']\n",
    "\n",
    "# Verificar que las columnas existen\n",
    "numerical_cols = [col for col in numerical_cols if col in df_encoded.columns]\n",
    "\n",
    "print(f\"\\nColumnas num√©ricas a normalizar ({len(numerical_cols)}):\")\n",
    "for col in numerical_cols:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# Aplicar StandardScaler\n",
    "df_encoded[numerical_cols] = scaler.fit_transform(df_encoded[numerical_cols])\n",
    "\n",
    "print(f\"\\n‚úì Normalizaci√≥n completada\")\n",
    "print(f\"  Media despu√©s de escalar (debe ser ~0): {df_encoded[numerical_cols].mean().mean():.6f}\")\n",
    "print(f\"  Desviaci√≥n est√°ndar (debe ser ~1): {df_encoded[numerical_cols].std().mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a12ae54",
   "metadata": {},
   "source": [
    "### Verificaci√≥n final de columnas antes de guardar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c10f68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VERIFICACI√ìN FINAL ANTES DE GUARDAR\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Verificar que no queden columnas tipo object (deben estar todas codificadas)\n",
    "object_cols = df_encoded.select_dtypes(include=['object']).columns.tolist()\n",
    "if object_cols:\n",
    "    print(f\"‚ö†Ô∏è  ADVERTENCIA: Quedan {len(object_cols)} columnas sin codificar:\")\n",
    "    for col in object_cols:\n",
    "        print(f\"  - {col}: {df_encoded[col].nunique()} valores √∫nicos\")\n",
    "else:\n",
    "    print(\"‚úÖ Todas las columnas categ√≥ricas han sido codificadas correctamente\")\n",
    "\n",
    "# Verificar que no queden valores faltantes (excepto si son intencionales)\n",
    "missing = df_encoded.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "if len(missing) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  ADVERTENCIA: Columnas con valores faltantes:\")\n",
    "    print(missing)\n",
    "else:\n",
    "    print(\"\\n‚úÖ No hay valores faltantes en el dataset\")\n",
    "\n",
    "# Convertir columnas booleanas a int (0 y 1)\n",
    "bool_cols = df_encoded.select_dtypes(include=['bool']).columns.tolist()\n",
    "if bool_cols:\n",
    "    df_encoded[bool_cols] = df_encoded[bool_cols].astype(int)\n",
    "    print(f\"‚úì Convertidas {len(bool_cols)} columnas booleanas a int64\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  No hay columnas booleanas para convertir\")\n",
    "\n",
    "print(\"\\nTipos de datos finales despu√©s de conversi√≥n:\")\n",
    "print(df_encoded.dtypes.value_counts())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPROBAR SI EXISTE EL DATASET LIMPIO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "output_path = \"data/diabetes_clean.csv\"\n",
    "\n",
    "# Validaci√≥n de la existencia del dataset limpio\n",
    "if os.path.exists(output_path):\n",
    "    print(f\"\\n‚úÖ El archivo ya existe: {output_path}\")\n",
    "else:\n",
    "    # Crear el archivo solo si no existe\n",
    "    df_encoded.to_csv(output_path, index=False)\n",
    "    print(f\"\\n‚úÖ Dataset limpio guardado exitosamente en: {output_path}\")\n",
    "    print(f\"   Dimensiones: {df_encoded.shape[0]:,} filas √ó {df_encoded.shape[1]} columnas\")\n",
    "\n",
    "# Verificaci√≥n final de la variable objetivo en el archivo guardado\n",
    "print(\"\\nüìä Verificaci√≥n de variable objetivo en archivo guardado:\")\n",
    "df_verification = pd.read_csv(output_path)\n",
    "target_counts = df_verification['readmitted_binary'].value_counts().sort_index()\n",
    "print(f\"   readmitted_binary:\")\n",
    "for class_val, count in target_counts.items():\n",
    "    pct = (count / len(df_verification)) * 100\n",
    "    print(f\"     Clase {class_val}: {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "# Validaci√≥n cr√≠tica\n",
    "expected_counts = df_encoded['readmitted_binary'].value_counts().sort_index()\n",
    "if target_counts.equals(expected_counts):\n",
    "    print(\"\\n‚úÖ VALIDACI√ìN EXITOSA: El archivo guardado contiene los datos correctos\")\n",
    "else:\n",
    "    print(\"\\n‚ùå ERROR: Discrepancia entre df_encoded y el archivo guardado\")\n",
    "    print(f\"   En memoria: {expected_counts.to_dict()}\")\n",
    "    print(f\"   En archivo: {target_counts.to_dict()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ PREPROCESAMIENTO COMPLETADO\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53778864",
   "metadata": {},
   "source": [
    "# An√°lisis Exploratorio de Datos (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351ce95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2000576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Cargar dataset limpio\n",
    "df = pd.read_csv(\"data/diabetes_clean.csv\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CARGA DEL DATASET LIMPIO\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"‚úì Dataset cargado: {df.shape[0]:,} filas √ó {df.shape[1]} columnas\")\n",
    "print(f\"‚úì Variable objetivo: readmitted_binary\")\n",
    "print(f\"   - Clase 0 (No readmitido <30): {(df['readmitted_binary']==0).sum():,} ({(df['readmitted_binary']==0).sum()/len(df)*100:.2f}%)\")\n",
    "print(f\"   - Clase 1 (Readmitido <30):    {(df['readmitted_binary']==1).sum():,} ({(df['readmitted_binary']==1).sum()/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165c92b4",
   "metadata": {},
   "source": [
    "### Parte 1: An√°lisis univariado-distribuciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a11501",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PARTE 1: AN√ÅLISIS UNIVARIADO - DISTRIBUCIONES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Variables num√©ricas principales (antes de normalizaci√≥n, las recuperamos del dataset original)\n",
    "df_original = pd.read_csv(\"data/diabetic_data.csv\")\n",
    "\n",
    "# Seleccionar variables num√©ricas clave\n",
    "numerical_vars = ['time_in_hospital', 'num_lab_procedures', 'num_procedures', \n",
    "                  'num_medications', 'number_diagnoses']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, var in enumerate(numerical_vars):\n",
    "    if var in df_original.columns:\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Histograma con KDE\n",
    "        ax.hist(df_original[var], bins=30, edgecolor='black', alpha=0.7, density=True)\n",
    "        \n",
    "        # Curva KDE\n",
    "        from scipy.stats import gaussian_kde\n",
    "        kde = gaussian_kde(df_original[var].dropna())\n",
    "        x_range = np.linspace(df_original[var].min(), df_original[var].max(), 100)\n",
    "        ax.plot(x_range, kde(x_range), 'r-', linewidth=2, label='KDE')\n",
    "        \n",
    "        # Estad√≠sticas\n",
    "        mean_val = df_original[var].mean()\n",
    "        median_val = df_original[var].median()\n",
    "        ax.axvline(mean_val, color='blue', linestyle='--', linewidth=2, label=f'Media: {mean_val:.2f}')\n",
    "        ax.axvline(median_val, color='green', linestyle='--', linewidth=2, label=f'Mediana: {median_val:.2f}')\n",
    "        \n",
    "        ax.set_title(f'Distribuci√≥n de {var}', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel(var)\n",
    "        ax.set_ylabel('Densidad')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Gr√°fico adicional: Distribuci√≥n de readmitted_binary (ya normalizado)\n",
    "ax = axes[5]\n",
    "readmit_counts = df['readmitted_binary'].value_counts().sort_index()\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "bars = ax.bar(['No Readmitido (<30)', 'Readmitido (<30)'], readmit_counts.values, \n",
    "              color=colors, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# A√±adir porcentajes en las barras\n",
    "for i, (bar, count) in enumerate(zip(bars, readmit_counts.values)):\n",
    "    height = bar.get_height()\n",
    "    pct = (count / len(df)) * 100\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{count:,}\\n({pct:.1f}%)',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_title('Distribuci√≥n de Variable Objetivo', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Cantidad de Pacientes')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7d4614",
   "metadata": {},
   "source": [
    "**Overview r√°pido**:\n",
    "El an√°lisis univariado revela patrones cl√≠nicos muy interesantes:\n",
    "- Primero, observamos que la mayor√≠a de hospitalizaciones son cortas (2-4 d√≠as) y conservadoras (0-1 procedimientos), lo que es t√≠pico de descompensaciones diab√©ticas manejadas m√©dicamente. (_fact-checked_)\n",
    "- Sin embargo, la polifarmacia es evidente: la mediana es de 15 medicamentos, y el promedio de 8 diagn√≥sticos refleja la alta complejidad de estos pacientes.\n",
    "- Crucialmente, nuestro dataset est√° significativamente desbalanceado (8:1), con solo el 11% de pacientes readmitidos en menos de 30 d√≠as. Esto nos obliga a usar t√©cnicas especializadas como SMOTE y priorizar m√©tricas como Recall sobre Accuracy, ya que los falsos negativos (pacientes en riesgo no detectados) tienen consecuencias graves tanto cl√≠nicas como econ√≥micas.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012d2863",
   "metadata": {},
   "source": [
    "### Parte 2: Agregaciones con ``.groupby()``- An√°lisis bivariado\n",
    "En esta secci√≥n exploramos la **relaci√≥n entre variables independientes y la readmisi√≥n hospitalaria (<30 d√≠as)**. El objetivo es identificar qu√© factores est√°n m√°s fuertemente asociados con el riesgo de readmisi√≥n para:\n",
    "\n",
    "1. **Priorizar features** en el modelado predictivo\n",
    "2. **Identificar grupos de alto riesgo** para intervenciones cl√≠nicas\n",
    "3. **Validar hip√≥tesis cl√≠nicas** con evidencia estad√≠stica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde6236f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PARTE 2: AGREGACIONES CON .groupby() - AN√ÅLISIS BIVARIADO\n",
    "# ============================================================================\n",
    "\n",
    "from scipy.stats import mannwhitneyu, chi2_contingency\n",
    "\n",
    "\n",
    "# Cargar datasets\n",
    "df = pd.read_csv(\"data/diabetes_clean.csv\")\n",
    "df_original = pd.read_csv(\"data/diabetic_data.csv\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PARTE 2: AGREGACIONES CON .groupby() - AN√ÅLISIS BIVARIADO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# PREPARACI√ìN: Sincronizar datasets\n",
    "# ============================================================================\n",
    "print(\"\\nüîß Sincronizando datasets...\")\n",
    "\n",
    "if 'readmitted_binary' not in df_original.columns:\n",
    "    df_original['readmitted_binary'] = df['readmitted_binary'].values\n",
    "    print(\"‚úì Columna 'readmitted_binary' a√±adida a df_original\")\n",
    "else:\n",
    "    print(\"‚úì Columna 'readmitted_binary' ya existe en df_original\")\n",
    "\n",
    "assert len(df_original) == len(df), \"ERROR: Los datasets tienen diferente n√∫mero de filas\"\n",
    "print(f\"‚úì Datasets sincronizados: {len(df_original):,} filas\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2.1 - TASA DE READMISI√ìN POR GRUPO DE EDAD\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"2.1 - TASA DE READMISI√ìN POR GRUPO DE EDAD\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Mapeo de c√≥digos de edad a etiquetas legibles\n",
    "age_mapping = {\n",
    "    0: '[0-10)', 1: '[10-20)', 2: '[20-30)', 3: '[30-40)', 4: '[40-50)',\n",
    "    5: '[50-60)', 6: '[60-70)', 7: '[70-80)', 8: '[80-90)', 9: '[90-100)'\n",
    "}\n",
    "\n",
    "# Crear columna de edad legible\n",
    "df['age_group'] = df['age'].map(age_mapping)\n",
    "\n",
    "# Calcular tasa de readmisi√≥n por edad\n",
    "readmit_by_age = df.groupby('age_group').agg({\n",
    "    'readmitted_binary': ['sum', 'count', 'mean']\n",
    "}).round(4)\n",
    "\n",
    "readmit_by_age.columns = ['Readmitidos', 'Total_Pacientes', 'Tasa_Readmision']\n",
    "readmit_by_age['Tasa_Readmision_Pct'] = (readmit_by_age['Tasa_Readmision'] * 100).round(2)\n",
    "readmit_by_age = readmit_by_age.sort_index()\n",
    "\n",
    "print(readmit_by_age)\n",
    "\n",
    "# Visualizaci√≥n\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 6))\n",
    "\n",
    "x_pos = np.arange(len(readmit_by_age))\n",
    "bars = ax.bar(x_pos, readmit_by_age['Tasa_Readmision_Pct'], \n",
    "              color='steelblue', edgecolor='black', linewidth=1.5)\n",
    "\n",
    "# Colorear barras seg√∫n tasa (rojo si >12%, amarillo si 10-12%, verde si <10%)\n",
    "for i, (bar, rate) in enumerate(zip(bars, readmit_by_age['Tasa_Readmision_Pct'])):\n",
    "    if rate > 12:\n",
    "        bar.set_color('#e74c3c')  # Rojo\n",
    "    elif rate > 10:\n",
    "        bar.set_color('#f39c12')  # Amarillo\n",
    "    else:\n",
    "        bar.set_color('#2ecc71')  # Verde\n",
    "\n",
    "# A√±adir valores en las barras\n",
    "for i, (bar, rate) in enumerate(zip(bars, readmit_by_age['Tasa_Readmision_Pct'])):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{rate:.2f}%',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Grupo de Edad', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Tasa de Readmisi√≥n (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Tasa de Readmisi√≥n (<30 d√≠as) por Grupo de Edad', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(readmit_by_age.index, rotation=45, ha='right')\n",
    "ax.axhline(y=readmit_by_age['Tasa_Readmision_Pct'].mean(), color='red', \n",
    "           linestyle='--', linewidth=2, label=f'Media Global: {readmit_by_age[\"Tasa_Readmision_Pct\"].mean():.2f}%')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# INSIGHT\n",
    "max_age = readmit_by_age['Tasa_Readmision_Pct'].idxmax()\n",
    "min_age = readmit_by_age['Tasa_Readmision_Pct'].idxmin()\n",
    "print(f\"\\nüìä INSIGHT:\")\n",
    "print(f\"   - Grupo con MAYOR tasa de readmisi√≥n: {max_age} ({readmit_by_age.loc[max_age, 'Tasa_Readmision_Pct']:.2f}%)\")\n",
    "print(f\"   - Grupo con MENOR tasa de readmisi√≥n: {min_age} ({readmit_by_age.loc[min_age, 'Tasa_Readmision_Pct']:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb12f45",
   "metadata": {},
   "source": [
    "El grupo de edad **[20-30) presenta la tasa M√ÅS ALTA de readmisi√≥n (14.24%)**, superando incluso a los ancianos de 80-90 a√±os (12.08%). Este resultado es **contrario a lo esperado**.\n",
    "#### Posibles Explicaciones Cl√≠nicas\n",
    "\n",
    "1. **Diabetes Tipo 1 no controlada**: M√°s com√∫n en j√≥venes, puede causar descompensaciones frecuentes (_Dunbar, et. al_)\n",
    "2. **Menor adherencia al tratamiento**: Pacientes j√≥venes pueden tener:\n",
    "   - Menor conciencia de la gravedad de su condici√≥n\n",
    "   - Dificultades para integrar el manejo de diabetes en su estilo de vida\n",
    "\n",
    "3. **Barreras socioecon√≥micas**: \n",
    "   - Menor acceso a seguro m√©dico\n",
    "   - Dificultades para acceder a atenci√≥n primaria post-alta\n",
    "   - Falta de recursos para medicamentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805a68a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2.2 - TASA DE READMISI√ìN POR RAZA Y G√âNERO\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"2.2 - TASA DE READMISI√ìN POR RAZA Y G√âNERO\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Mapeo de c√≥digos de raza y g√©nero\n",
    "race_mapping = {0: 'AfricanAmerican', 1: 'Asian', 2: 'Caucasian', 3: 'Hispanic', 4: 'Other'}\n",
    "gender_mapping = {0: 'Female', 1: 'Male', 2: 'Unknown/Invalid'}\n",
    "\n",
    "df['race_label'] = df['race'].map(race_mapping)\n",
    "df['gender_label'] = df['gender'].map(gender_mapping)\n",
    "\n",
    "# Tabla cruzada: Raza √ó G√©nero\n",
    "readmit_by_race_gender = df.groupby(['race_label', 'gender_label']).agg({\n",
    "    'readmitted_binary': ['sum', 'count', 'mean']\n",
    "}).round(4)\n",
    "\n",
    "readmit_by_race_gender.columns = ['Readmitidos', 'Total', 'Tasa']\n",
    "readmit_by_race_gender['Tasa_Pct'] = (readmit_by_race_gender['Tasa'] * 100).round(2)\n",
    "\n",
    "print(readmit_by_race_gender)\n",
    "\n",
    "# Visualizaci√≥n: Heatmap\n",
    "pivot_table = readmit_by_race_gender['Tasa_Pct'].unstack(fill_value=0)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "sns.heatmap(pivot_table, annot=True, fmt='.2f', cmap='RdYlGn_r', \n",
    "            linewidths=1, linecolor='black', cbar_kws={'label': 'Tasa de Readmisi√≥n (%)'}, ax=ax)\n",
    "ax.set_title('Tasa de Readmisi√≥n (<30 d√≠as) por Raza y G√©nero (%)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('G√©nero', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Raza', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Test Chi-cuadrado para raza\n",
    "print(\"\\nüìä Test Chi-cuadrado: Raza vs Readmisi√≥n\")\n",
    "contingency_race = pd.crosstab(df['race_label'], df['readmitted_binary'])\n",
    "chi2_race, pvalue_race, dof_race, expected_race = chi2_contingency(contingency_race)\n",
    "print(f\"   p-value: {pvalue_race:.4f}\")\n",
    "if pvalue_race < 0.05:\n",
    "    print(\"   ‚úì Diferencias significativas entre razas\")\n",
    "else:\n",
    "    print(\"   ‚úó Diferencias NO significativas entre razas\")\n",
    "\n",
    "# Test Chi-cuadrado para g√©nero\n",
    "print(\"\\nüìä Test Chi-cuadrado: G√©nero vs Readmisi√≥n\")\n",
    "contingency_gender = pd.crosstab(df['gender_label'], df['readmitted_binary'])\n",
    "chi2_gender, pvalue_gender, dof_gender, expected_gender = chi2_contingency(contingency_gender)\n",
    "print(f\"   p-value: {pvalue_gender:.4f}\")\n",
    "if pvalue_gender < 0.05:\n",
    "    print(\"   ‚úì Diferencias significativas entre g√©neros\")\n",
    "else:\n",
    "    print(\"   ‚úó Diferencias NO significativas entre g√©neros\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672003fc",
   "metadata": {},
   "source": [
    "Aunque no son estad√≠sticamente significativas, observamos variaciones moderadas:\n",
    "- **Rango de tasas por raza**: 7.55% (Asian Female) - 12.69% (Asian Male)\n",
    "- **Rango de tasas por g√©nero**: 8.98% (Other Male) - 11.85% (Hispanic Male)\n",
    "- **Diferencia m√°xima**: ~5 puntos porcentuales\n",
    "\n",
    "La poblaci√≥n **Asi√°tica** tiene una muestra peque√±a (n=641, 0.6% del total), lo que puede explicar:\n",
    "- Mayor variabilidad en las tasas (7.55% - 12.69%)\n",
    "- Menor confiabilidad de las estimaciones\n",
    "- Posible influencia de outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa095f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2.3 - TIEMPO DE HOSPITALIZACI√ìN SEG√öN READMISI√ìN\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"2.3 - TIEMPO DE HOSPITALIZACI√ìN SEG√öN READMISI√ìN\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# A√±adir label de readmisi√≥n a df_original\n",
    "df_original['readmit_label'] = df['readmitted_binary'].map({0: 'No Readmitido', 1: 'Readmitido (<30 d√≠as)'})\n",
    "\n",
    "time_by_readmit = df_original.groupby('readmit_label')['time_in_hospital'].describe()\n",
    "print(time_by_readmit)\n",
    "\n",
    "# Visualizaci√≥n\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Subplot 1: Boxplot\n",
    "ax = axes[0]\n",
    "sns.boxplot(data=df_original, x='readmit_label', y='time_in_hospital', \n",
    "            palette=['#2ecc71', '#e74c3c'], ax=ax)\n",
    "ax.set_title('Tiempo de Hospitalizaci√≥n seg√∫n Readmisi√≥n', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Estado de Readmisi√≥n', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('D√≠as en Hospital', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Subplot 2: Violinplot\n",
    "ax = axes[1]\n",
    "sns.violinplot(data=df_original, x='readmit_label', y='time_in_hospital', \n",
    "               palette=['#2ecc71', '#e74c3c'], ax=ax)\n",
    "ax.set_title('Distribuci√≥n del Tiempo de Hospitalizaci√≥n', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Estado de Readmisi√≥n', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('D√≠as en Hospital', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Test estad√≠stico\n",
    "group_0 = df_original[df_original['readmitted_binary'] == 0]['time_in_hospital']\n",
    "group_1 = df_original[df_original['readmitted_binary'] == 1]['time_in_hospital']\n",
    "statistic, pvalue = mannwhitneyu(group_0, group_1, alternative='two-sided')\n",
    "\n",
    "print(f\"\\nüìä INSIGHT - Test de Mann-Whitney U:\")\n",
    "print(f\"   - Estad√≠stico U: {statistic:.2f}\")\n",
    "print(f\"   - p-value: {pvalue:.4e}\")\n",
    "if pvalue < 0.001:\n",
    "    print(f\"   ‚úì Diferencia ESTAD√çSTICAMENTE SIGNIFICATIVA (p < 0.001)\")\n",
    "    print(f\"   ‚ö†Ô∏è  Pero la diferencia cl√≠nica es PEQUE√ëA: {time_by_readmit.loc['Readmitido (<30 d√≠as)', 'mean'] - time_by_readmit.loc['No Readmitido', 'mean']:.2f} d√≠as\")\n",
    "else:\n",
    "    print(f\"   ‚úó Sin diferencia significativa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7dbd94",
   "metadata": {},
   "source": [
    "| Aspecto | Resultado |\n",
    "|---------|-----------|\n",
    "| **Diferencia observada** | 0.42 d√≠as (~10 horas) |\n",
    "| **Significancia estad√≠stica** | p < 0.001 (S√ç significativo) |\n",
    "| **Relevancia cl√≠nica** | BAJA (diferencia peque√±a) |\n",
    "\n",
    "Con una muestra de **101,766 pacientes**, incluso diferencias **muy peque√±as** resultan estad√≠sticamente significativas debido al alto poder estad√≠stico. Sin embargo, una diferencia de **medio d√≠a de estancia** es cl√≠nicamente trivial.\n",
    "\n",
    "Los boxplots y violinplots muestran:\n",
    "- **Misma mediana**: 4 d√≠as en ambos grupos\n",
    "- **Mismo rango intercuart√≠lico**: 2-6 d√≠as\n",
    "- **Distribuciones muy superpuestas**: La mayor√≠a de pacientes tiene 2-6 d√≠as independientemente de si fueron readmitidos\n",
    "\n",
    "\n",
    "**El tiempo de hospitalizaci√≥n por s√≠ solo NO es un predictor fuerte**, pero puede interactuar con otras variables:\n",
    "- Estancias **muy cortas** (<2 d√≠as): ¬øEstamos hablando de un alta prematura?\n",
    "- Estancias **muy largas** (>10 d√≠as): Indicador de complicaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3a57f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2.4 - N√öMERO DE MEDICAMENTOS Y READMISI√ìN\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"2.4 - N√öMERO DE MEDICAMENTOS Y READMISI√ìN\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Crear DataFrame temporal\n",
    "df_temp_meds = pd.DataFrame({\n",
    "    'num_medications': df_original['num_medications'],\n",
    "    'readmitted_binary': df['readmitted_binary']\n",
    "})\n",
    "\n",
    "# Crear bins de medicamentos\n",
    "df_temp_meds['num_medications_bin'] = pd.cut(df_temp_meds['num_medications'], \n",
    "                                              bins=[0, 10, 15, 20, 25, 100],\n",
    "                                              labels=['1-10', '11-15', '16-20', '21-25', '>25'])\n",
    "\n",
    "readmit_by_meds = df_temp_meds.groupby('num_medications_bin').agg({\n",
    "    'readmitted_binary': ['sum', 'count', 'mean']\n",
    "})\n",
    "readmit_by_meds.columns = ['Readmitidos', 'Total', 'Tasa']\n",
    "readmit_by_meds['Tasa_Pct'] = (readmit_by_meds['Tasa'] * 100).round(2)\n",
    "\n",
    "print(readmit_by_meds)\n",
    "\n",
    "# Visualizaci√≥n\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "x_pos = np.arange(len(readmit_by_meds))\n",
    "bars = ax.bar(x_pos, readmit_by_meds['Tasa_Pct'], color='coral', edgecolor='black', linewidth=1.5)\n",
    "\n",
    "for i, (bar, rate) in enumerate(zip(bars, readmit_by_meds['Tasa_Pct'])):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{rate:.2f}%',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('N√∫mero de Medicamentos', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Tasa de Readmisi√≥n (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Tasa de Readmisi√≥n seg√∫n N√∫mero de Medicamentos', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(readmit_by_meds.index)\n",
    "ax.axhline(y=readmit_by_meds['Tasa_Pct'].mean(), color='red', \n",
    "           linestyle='--', linewidth=2, label=f'Media: {readmit_by_meds[\"Tasa_Pct\"].mean():.2f}%')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# An√°lisis de tendencia\n",
    "print(f\"\\nüìä INSIGHT:\")\n",
    "min_rate = readmit_by_meds['Tasa_Pct'].min()\n",
    "max_rate = readmit_by_meds['Tasa_Pct'].max()\n",
    "diff_pct = ((max_rate - min_rate) / min_rate) * 100\n",
    "print(f\"   - Incremento de riesgo: {diff_pct:.1f}% (de {min_rate:.2f}% a {max_rate:.2f}%)\")\n",
    "print(f\"   - Tendencia: MONOT√ìNICA CRECIENTE (a m√°s medicamentos ‚Üí mayor riesgo)\")\n",
    "print(f\"   - Umbral cr√≠tico: >15 medicamentos (tasa >12%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d853676",
   "metadata": {},
   "source": [
    "| Medicamentos | Tasa Readmisi√≥n| Incremento vs Baseline | Interpretaci√≥n |\n",
    "|--------------|------|------------------------|----------------|\n",
    "| 1-10 | 9.06% | ‚Äî | Baseline (casos simples) |\n",
    "| 11-15 | 10.91% | +20% | Complejidad moderada |\n",
    "| 16-20 | 12.17% | +34% | Alta complejidad |\n",
    "| **21-25** | **12.99%** | **+43%** | Riesgo m√°ximo |\n",
    "| >25 | 12.54% | +38% | Ligera disminuci√≥n (plateau) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a860a3",
   "metadata": {},
   "source": [
    "El n√∫mero de medicamentos refleja:\n",
    "\n",
    "1. **Carga de comorbilidades**: M√°s enfermedades cr√≥nicas = m√°s f√°rmacos\n",
    "2. **Fragilidad del paciente**: M√∫ltiples sistemas afectados\n",
    "3. **Riesgo de interacciones**: A m√°s f√°rmacos, mayor probabilidad de:\n",
    "   - Interacciones medicamentosas\n",
    "   - Efectos adversos\n",
    "   - Errores de administraci√≥n\n",
    "4. **Menor adherencia**: La complejidad del r√©gimen terap√©utico dificulta el cumplimiento\n",
    "\n",
    "#### ¬øPor qu√© disminuye levemente en el caso de pacientes con >25 medicamentos?\n",
    "\n",
    "Posibles explicaciones:\n",
    "- **Sesgo de supervivencia**: Pacientes muy complejos pero bajo manejo especializado intensivo\n",
    "- **Mayor vigilancia**: Casos con >25 f√°rmacos probablemente tienen seguimiento por m√∫ltiples especialistas\n",
    "- **Muestra m√°s peque√±a**: Solo 11,359 pacientes (11%) con >25 medicamentos\n",
    "\n",
    "#### Umbral Cr√≠tico Identificado: >15 Medicamentos\n",
    "\n",
    "Pacientes con **m√°s de 15 medicamentos** cruzan un umbral de riesgo:\n",
    "- Tasa salta de 10.91% ‚Üí 12.17% (+11.5%)\n",
    "- Se mantiene elevada hasta >25\n",
    "- **Recomendaci√≥n cl√≠nica**: Priorizar seguimiento farmacoterap√©utico\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09549cf",
   "metadata": {},
   "source": [
    "## Parte 3: Matriz de Correlaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d5014a",
   "metadata": {},
   "source": [
    "**Observamos varias cosas**<br><br>\n",
    "- **Ninguna variable individual tiene correlaci√≥n lineal fuerte con readmisi√≥n** (todas <0.20). Esto indica:\n",
    "\n",
    "    1.  **La readmisi√≥n es un fen√≥meno multifactorial**: No existe un √∫nico predictor dominante\n",
    "    2.  **Relaciones no lineales**: Variables como `num_medications` y `age` mostraron patrones claros en an√°lisis bivariado pero correlaci√≥n lineal d√©bil\n",
    "    3.  **Interacciones complejas**: El riesgo de readmisi√≥n depende de combinaciones de factores, no de variables aisladas\n",
    "\n",
    "\n",
    "\n",
    "- `total_visits` es la **suma** de sus componentes:\n",
    "    ```python\n",
    "    total_visits = number_inpatient + number_emergency + number_outpatient\n",
    "    ```\n",
    "De all√≠ la alta correlaci√≥n.\n",
    "\n",
    "- #### Correlaciones Esperadas (Coherencia Cl√≠nica)\n",
    "\n",
    "| Par de Variables | r | Interpretaci√≥n |\n",
    "|------------------|---|----------------|\n",
    "| **time_in_hospital ‚Üî num_medications** | **0.47** | Estancias largas ‚Üí M√°s medicamentos (complejidad) |\n",
    "| **time_in_hospital ‚Üî num_procedures** | **0.19** | Estancias largas ‚Üí M√°s procedimientos |\n",
    "| **time_in_hospital ‚Üî num_lab_procedures** | **0.32** | Estancias largas ‚Üí M√°s laboratorios |\n",
    "| **num_medications ‚Üî num_procedures** | **0.39** | M√°s procedimientos ‚Üí M√°s f√°rmacos |\n",
    "| **num_medications ‚Üî number_diagnoses** | **0.26** | M√°s diagn√≥sticos ‚Üí M√°s medicamentos |\n",
    "\n",
    "‚úÖ **Estas correlaciones son esperables** y reflejan la **complejidad cl√≠nica** de forma coherente. No representan un problema de multicolinealidad grave (<0.50).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db18d64",
   "metadata": {},
   "source": [
    "## Parte 4: An√°lisis adicional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4381e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PARTE 4: AN√ÅLISIS ADICIONALES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# 4.1 - DISTRIBUCI√ìN DE DIAGN√ìSTICOS (TOP 10 CATEGOR√çAS ICD-9)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"4.1 - TOP 10 CATEGOR√çAS DE DIAGN√ìSTICO PRIMARIO (ICD-9)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Mapeo de categor√≠as ICD-9\n",
    "diag_mapping = {\n",
    "    0: 'Blood_Diseases', 1: 'Circulatory_Heart_Failure', 2: 'Circulatory_Ischemic_Heart',\n",
    "    3: 'Circulatory_Other', 4: 'Congenital_Anomalies', 5: 'Diabetes',\n",
    "    6: 'Digestive', 7: 'Endocrine_Metabolic', 8: 'External_Causes',\n",
    "    9: 'Genitourinary_Other', 10: 'Genitourinary_Renal', 11: 'Infectious_Parasitic',\n",
    "    12: 'Injury_Poisoning', 13: 'Mental_Disorders', 14: 'Musculoskeletal',\n",
    "    15: 'Neoplasms', 16: 'Nervous_Sense', 17: 'Other', 18: 'Respiratory_COPD',\n",
    "    19: 'Respiratory_Other', 20: 'Respiratory_Pneumonia', 21: 'Skin_Subcutaneous',\n",
    "    22: 'Symptoms_Signs', 23: 'Unknown'\n",
    "}\n",
    "\n",
    "# Verificar si existe diag_1_category\n",
    "if 'diag_1_category' in df.columns:\n",
    "    df['diag_1_label'] = df['diag_1_category'].map(diag_mapping)\n",
    "    \n",
    "    top_diag = df['diag_1_label'].value_counts().head(10)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 7))\n",
    "    bars = ax.barh(top_diag.index, top_diag.values, color='teal', edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    for i, (bar, val) in enumerate(zip(bars, top_diag.values)):\n",
    "        width = bar.get_width()\n",
    "        ax.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "                f'{val:,} ({(val/len(df)*100):.1f}%)',\n",
    "                ha='left', va='center', fontsize=10, fontweight='bold', \n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))\n",
    "    \n",
    "    ax.set_xlabel('Cantidad de Pacientes', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('TOP 10 Categor√≠as de Diagn√≥stico Primario (ICD-9)', fontsize=14, fontweight='bold')\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "# ============================================================================\n",
    "# 4.2 - READMISI√ìN POR DIAGN√ìSTICO PRIMARIO (TOP 10)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"4.2 - TASA DE READMISI√ìN POR DIAGN√ìSTICO PRIMARIO\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if 'diag_1_label' in df.columns:\n",
    "    readmit_by_diag = df.groupby('diag_1_label').agg({\n",
    "        'readmitted_binary': ['sum', 'count', 'mean']\n",
    "    })\n",
    "    readmit_by_diag.columns = ['Readmitidos', 'Total', 'Tasa']\n",
    "    readmit_by_diag['Tasa_Pct'] = (readmit_by_diag['Tasa'] * 100).round(2)\n",
    "    \n",
    "    # Filtrar diagn√≥sticos con al menos 1000 pacientes\n",
    "    readmit_by_diag_filtered = readmit_by_diag[readmit_by_diag['Total'] >= 1000].sort_values('Tasa_Pct', ascending=False).head(10)\n",
    "    \n",
    "    print(readmit_by_diag_filtered)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 7))\n",
    "    bars = ax.barh(readmit_by_diag_filtered.index, readmit_by_diag_filtered['Tasa_Pct'], \n",
    "                   color='salmon', edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    for i, (bar, rate) in enumerate(zip(bars, readmit_by_diag_filtered['Tasa_Pct'])):\n",
    "        width = bar.get_width()\n",
    "        total = readmit_by_diag_filtered.iloc[i]['Total']\n",
    "        ax.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "                f'{rate:.2f}% (n={total:,})',\n",
    "                ha='left', va='center', fontsize=10, fontweight='bold',\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))\n",
    "    \n",
    "    ax.set_xlabel('Tasa de Readmisi√≥n (%)', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('TOP 10 Diagn√≥sticos con Mayor Tasa de Readmisi√≥n (>1000 pacientes)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    ax.axvline(x=readmit_by_diag['Tasa_Pct'].mean(), color='red', \n",
    "               linestyle='--', linewidth=2, label=f'Media Global: {readmit_by_diag[\"Tasa_Pct\"].mean():.2f}%')\n",
    "    ax.legend()\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "# ============================================================================\n",
    "# 4.3 - USO DE INSULINA Y READMISI√ìN\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"-\" * 80)\n",
    "print(\"4.3 - USO DE INSULINA Y READMISI√ìN\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Mapeo de insulin\n",
    "insulin_mapping = {0: 'Down', 1: 'No', 2: 'Steady', 3: 'Up'}\n",
    "\n",
    "if 'insulin' in df.columns:\n",
    "    df['insulin_label'] = df['insulin'].map(insulin_mapping)\n",
    "    \n",
    "    readmit_by_insulin = df.groupby('insulin_label').agg({\n",
    "        'readmitted_binary': ['sum', 'count', 'mean']\n",
    "    })\n",
    "    readmit_by_insulin.columns = ['Readmitidos', 'Total', 'Tasa']\n",
    "    readmit_by_insulin['Tasa_Pct'] = (readmit_by_insulin['Tasa'] * 100).round(2)\n",
    "    \n",
    "    print(readmit_by_insulin)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    \n",
    "    colors_insulin = {'No': '#95a5a6', 'Steady': '#3498db', 'Up': '#2ecc71', 'Down': '#e74c3c'}\n",
    "    order = ['No', 'Steady', 'Up', 'Down']\n",
    "    order = [o for o in order if o in readmit_by_insulin.index]\n",
    "    \n",
    "    bars = ax.bar(order, [readmit_by_insulin.loc[o, 'Tasa_Pct'] for o in order],\n",
    "                  color=[colors_insulin.get(x, 'steelblue') for x in order],\n",
    "                  edgecolor='black', linewidth=1.5)\n",
    "    \n",
    "    for bar, label in zip(bars, order):\n",
    "        height = bar.get_height()\n",
    "        total = readmit_by_insulin.loc[label, 'Total']\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.2f}%\\n(n={total:,})',\n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    ax.set_xlabel('Uso de Insulina', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Tasa de Readmisi√≥n (%)', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Tasa de Readmisi√≥n seg√∫n Uso de Insulina', fontsize=14, fontweight='bold')\n",
    "    ax.axhline(y=readmit_by_insulin['Tasa_Pct'].mean(), color='red',\n",
    "               linestyle='--', linewidth=2, label=f'Media: {readmit_by_insulin[\"Tasa_Pct\"].mean():.2f}%')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ PARTE 4 COMPLETADA\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a378ffd",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "\n",
    "# ROBERT\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27250cac",
   "metadata": {},
   "source": [
    "# Modelado y Clasificaci√≥n\n",
    "\n",
    "En esta fase abordamos el problema de clasificaci√≥n binaria para predecir si un paciente ser√° readmitido en el hospital en menos de 30 d√≠as. \n",
    "\n",
    "Dado el **desbalance de clases** identificado previamente (~11% de readmisiones), utilizaremos la estrategia `class_weight='balanced'` en los siguentes modelos \"cl√°sicos\". Esta t√©cnica ajusta autom√°ticamente los pesos de las clases de forma inversamente proporcional a su frecuencia, lo que permite al algoritmo \"prestar m√°s atenci√≥n\" a la clase minoritaria sin necesidad de t√©cnicas de remuestreo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695d5654",
   "metadata": {},
   "source": [
    "## Preparaci√≥n de datos para el modelado\n",
    "\n",
    "Antes de entrenar cualquier modelo, preparamos los datos siguiendo estos pasos:\n",
    "\n",
    "1. **Definici√≥n de features (X) y target (y):** separamos la variable objetivo `readmitted_binary` de las predictoras, eliminando identificadores y columnas redundantes.\n",
    "2. **Codificaci√≥n (One-Hot encoding):** convertimos variables categ√≥ricas a num√©ricas con `get_dummies`.\n",
    "3. **Divisi√≥n train/test (80/20):** utilizamos `stratify=y` para mantener la proporci√≥n de clases en ambos conjuntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1146193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Columnas a excluir (identificadores, target y columnas redundantes)\n",
    "cols_to_exclude = [\n",
    "    'encounter_id', 'patient_nbr',             \n",
    "    'readmitted_binary',         \n",
    "    'readmit_label',                           \n",
    "    'insulin_label', 'num_medications_bin',    \n",
    "    'diag_1_label'                             \n",
    "]\n",
    "\n",
    "existing_cols_to_drop = [c for c in cols_to_exclude if c in df.columns]\n",
    "X = df.drop(columns=existing_cols_to_drop)\n",
    "y = df['readmitted_binary']\n",
    "\n",
    "# Codificaci√≥n One-Hot\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Divisi√≥n estratificada\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Dimensiones - train: {X_train.shape} | test: {X_test.shape}\")\n",
    "print(f\"Proporci√≥n readmitidos - train: {y_train.mean():.2%} | test: {y_test.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351548eb",
   "metadata": {},
   "source": [
    "La divisi√≥n mantiene la proporci√≥n original de clases (~11% readmitidos) tanto en train como en test, gracias al par√°metro `stratify`. Esto es crucial para una evaluaci√≥n realista del modelo.\n",
    "\n",
    "Esto es **lo que necesitamos**, porque:\n",
    "\n",
    "1. Se realiza una evaluaci√≥n realista: el test set refleja la misma distribuci√≥n que encontrar√°s en producci√≥n\n",
    "2. Evita sesgos: sin stratify, podr√≠as tener por azar 15% de readmitidos en train y 8% en test, lo que distorsionar√≠a las m√©tricas\n",
    "3. Alta reproducibilidad: cada partici√≥n es representativa del problema real"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2161b77d",
   "metadata": {},
   "source": [
    "## Regresi√≥n Log√≠stica (Baseline)\n",
    "\n",
    "Comenzamos con **Regresi√≥n Log√≠stica** como modelo baseline por su simplicidad e interpretabilidad. Este modelo lineal estima la probabilidad de readmisi√≥n bas√°ndose en una combinaci√≥n ponderada de las features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbf6368",
   "metadata": {},
   "source": [
    "### Entrenamiento y configuraci√≥n visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5974a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Configuraci√≥n de estilo para gr√°ficas\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({\n",
    "    'figure.facecolor': 'white',\n",
    "    'axes.facecolor': '#f8f9fa',\n",
    "    'axes.edgecolor': '#333333',\n",
    "    'font.size': 11,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12\n",
    "})\n",
    "\n",
    "# Paleta de colores\n",
    "COLORS = {\n",
    "    'primary': '#2E86AB', 'secondary': '#A23B72', 'success': '#28A745',\n",
    "    'danger': '#DC3545', 'warning': '#FFC107', 'info': '#17A2B8', 'dark': '#343A40'\n",
    "}\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "log_reg = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3631b3a7",
   "metadata": {},
   "source": [
    "El output que vemos tras ejecutar `log_reg.fit(X_train, y_train)` es simplemente la representaci√≥n del objetivo del modelo.\n",
    "\n",
    "¬øQu√© significa cada par√°metro?\n",
    "\n",
    "| Par√°metro | Valor | Significado |\n",
    "| :--- | :--- | :--- |\n",
    "| `class_weight='balanced'` | `'balanced'` | Ajusta autom√°ticamente los pesos para compensar el desbalance de clases (89% vs 11%) |\n",
    "| `max_iter=1000` | `1000` | N√∫mero m√°ximo de iteraciones para que el algoritmo converja |\n",
    "| `n_jobs=-1` | `-1` | Usa todos los n√∫cleos del CPU disponibles para paralelizar |\n",
    "| `random_state=42` | `42` | Semilla para reproducibilidad |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1591f6",
   "metadata": {},
   "source": [
    "### Validaci√≥n cruzada\n",
    "\n",
    "Evaluamos la estabilidad del modelo con validaci√≥n cruzada de 5 particiones. Esto nos da una estimaci√≥n m√°s robusta del rendimiento que una simple divisi√≥n train/test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9c0819",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores_acc = cross_val_score(log_reg, X_train, y_train, cv=5, scoring='accuracy')\n",
    "cv_scores_f1 = cross_val_score(log_reg, X_train, y_train, cv=5, scoring='f1')\n",
    "cv_scores_recall = cross_val_score(log_reg, X_train, y_train, cv=5, scoring='recall')\n",
    "\n",
    "print(f\"Accuracy:  {cv_scores_acc.mean():.4f} ¬± {cv_scores_acc.std():.4f}\")\n",
    "print(f\"F1-Score:  {cv_scores_f1.mean():.4f} ¬± {cv_scores_f1.std():.4f}\")\n",
    "print(f\"Recall:    {cv_scores_recall.mean():.4f} ¬± {cv_scores_recall.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b323ff",
   "metadata": {},
   "source": [
    "Los resultados muestran baja variabilidad entre folds (desviaciones est√°ndar peque√±as), lo que indica que el modelo es estable. El f1-Score de ~0.25 y recall de ~0.53 reflejan las dificultades inherentes a predecir readmisiones, un problema conocido por su complejidad.\n",
    "\n",
    "### Evaluaci√≥n en test set y matriz de confusi√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b1b944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones en test\n",
    "y_pred_log = log_reg.predict(X_test)\n",
    "y_prob_log = log_reg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(classification_report(y_test, y_pred_log, target_names=['No Readmitido', 'Readmitido']))\n",
    "\n",
    "# Matriz de Confusi√≥n\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred_log)\n",
    "cm_pct = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "\n",
    "sns.heatmap(cm, annot=False, cmap='Blues', cbar=True, ax=ax, linewidths=2, linecolor='white', square=True)\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        text_color = 'white' if cm[i, j] > cm.max()/2 else 'black'\n",
    "        ax.text(j + 0.5, i + 0.5, f'{cm[i,j]:,}\\n({cm_pct[i,j]:.1f}%)', \n",
    "                ha='center', va='center', fontsize=14, fontweight='bold', color=text_color)\n",
    "\n",
    "ax.set_xlabel('Predicci√≥n', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Valor Real', fontsize=12, fontweight='bold')\n",
    "ax.set_xticklabels(['No Readmitido', 'Readmitido'], fontsize=11)\n",
    "ax.set_yticklabels(['No Readmitido', 'Readmitido'], fontsize=11, rotation=0)\n",
    "ax.set_title('Matriz de Confusi√≥n - Regresi√≥n Log√≠stica', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23cf25c",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n de la matriz:**\n",
    "- **Verdaderos negativos (12,280):** pacientes no readmitidos correctamente clasificados\n",
    "- **Falsos positivos (5,803):** pacientes no readmitidos clasificados err√≥neamente como readmitidos\n",
    "- **Falsos negativos (1,073):** pacientes readmitidos que el modelo no detect√≥ (los m√°s cr√≠ticos en contexto m√©dico)\n",
    "- **Verdaderos positivos (1,198):** pacientes readmitidos correctamente identificados\n",
    "\n",
    "El modelo detecta aproximadamente el 53% de los readmitidos (recall), pero con una precisi√≥n baja (17%), generando muchos falsos positivos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7762fe0d",
   "metadata": {},
   "source": [
    "### An√°lisis de umbral de decisi√≥n\n",
    "\n",
    "Por defecto, clasificamos como positivo cuando P(readmitido) > 0.5. En datos desbalanceados, ajustar este umbral puede mejorar el balance entre precisi√≥n y recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa6fd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "results_thresh = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    y_pred_thresh = (y_prob_log >= thresh).astype(int)\n",
    "    report = classification_report(y_test, y_pred_thresh, output_dict=True, zero_division=0)\n",
    "    results_thresh.append({\n",
    "        'Umbral': thresh, 'Precision': report['1']['precision'],\n",
    "        'Recall': report['1']['recall'], 'F1-Score': report['1']['f1-score']\n",
    "    })\n",
    "\n",
    "# Gr√°fica\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "precisions = [r['Precision'] for r in results_thresh]\n",
    "recalls = [r['Recall'] for r in results_thresh]\n",
    "f1_scores = [r['F1-Score'] for r in results_thresh]\n",
    "\n",
    "ax.plot(thresholds, precisions, 'o-', color=COLORS['primary'], linewidth=2.5, markersize=8, label='Precision')\n",
    "ax.plot(thresholds, recalls, 's-', color=COLORS['danger'], linewidth=2.5, markersize=8, label='Recall')\n",
    "ax.plot(thresholds, f1_scores, '^-', color=COLORS['success'], linewidth=2.5, markersize=8, label='F1-Score')\n",
    "ax.axvline(x=0.5, color='gray', linestyle='--', linewidth=1.5, alpha=0.7, label='Umbral por defecto')\n",
    "\n",
    "ax.set_xlabel('Umbral de Decisi√≥n', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Impacto del Umbral de Decisi√≥n en las M√©tricas', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "ax.set_xlim(0.15, 0.75)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47cdb6d",
   "metadata": {},
   "source": [
    "**Observaciones:**\n",
    "- Con umbral **bajo (0.2-0.3)**: recall cercano al 100% (detectamos casi todos los readmitidos), pero Precision muy baja (~11%)\n",
    "- Con umbral **alto (0.6-0.7)**: precision aumenta (~29%) pero recall cae dr√°sticamente (~11%)\n",
    "- El umbral **0.5** ofrece un compromiso razonable con el mejor F1-Score\n",
    "\n",
    "En contexto m√©dico, podr√≠a preferirse un umbral m√°s bajo para no dejar escapar pacientes en riesgo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6258372",
   "metadata": {},
   "source": [
    "### Curva ROC y AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be914b3",
   "metadata": {},
   "source": [
    "La **Curva ROC (Receiver Operating Characteristic)** es una representaci√≥n gr√°fica que ilustra la capacidad de diagn√≥stico de un sistema clasificador binario a medida que se var√≠a su umbral de discriminaci√≥n.\n",
    "\n",
    "*   **Eje Y (Sensibilidad o TPR):** representa la tasa de verdaderos positivos. ¬øQu√© porcentaje de los pacientes realmente readmitidos fuimos capaces de detectar?\n",
    "*   **Eje X (1 - Especificidad o FPR):** representa la tasa de falsos positivos. ¬øQu√© porcentaje de pacientes sanos (no readmitidos) clasificamos err√≥neamente como en riesgo?\n",
    "\n",
    "El **AUC (Area Under the Curve)** es el √°rea bajo esta curva y proporciona una m√©trica √∫nica para comparar modelos:\n",
    "*   **0.5:** predicci√≥n aleatoria (sin valor).\n",
    "*   **0.7 - 0.8:** rendimiento aceptable.\n",
    "*   **> 0.8:** rendimiento excelente.\n",
    "\n",
    "Un AUC de **0.645** (como veremos abajo) significa que si tomamos un paciente readmitido y uno no readmitido al azar, el modelo clasificar√° correctamente al readmitido con mayor probabilidad el 64.5% de las veces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293a7acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_test, y_prob_log)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "ax.fill_between(fpr, tpr, alpha=0.3, color=COLORS['primary'])\n",
    "ax.plot(fpr, tpr, color=COLORS['primary'], lw=3, label=f'Regresi√≥n Log√≠stica (AUC = {roc_auc:.3f})')\n",
    "ax.plot([0, 1], [0, 1], color=COLORS['dark'], lw=2, linestyle='--', label='Clasificador Aleatorio')\n",
    "\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "ax.scatter(fpr[optimal_idx], tpr[optimal_idx], s=150, c=COLORS['warning'], edgecolors='black', linewidths=2, zorder=5)\n",
    "\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('Tasa de Falsos Positivos (FPR)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Tasa de Verdaderos Positivos (TPR)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Curva ROC - Regresi√≥n Log√≠stica', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98280d7",
   "metadata": {},
   "source": [
    "El **AUC de 0.645** indica capacidad discriminativa moderada (superior al azar = 0.5 pero lejos de un clasificador perfecto = 1.0). El punto amarillo marca el umbral √≥ptimo seg√∫n el estad√≠stico de Youden (m√°ximo TPR - FPR).\n",
    "\n",
    "### Importancia de Features\n",
    "\n",
    "Los coeficientes del modelo nos indican qu√© variables tienen mayor influencia en la predicci√≥n de readmisi√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b44d3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pd.Series(log_reg.coef_[0], index=X.columns)\n",
    "top_features = coefs.abs().sort_values(ascending=False).head(10)\n",
    "top_features_original = coefs[top_features.index].sort_values()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "colors = [COLORS['success'] if x > 0 else COLORS['danger'] for x in top_features_original]\n",
    "bars = ax.barh(range(len(top_features_original)), top_features_original.values, color=colors, edgecolor='white', height=0.7)\n",
    "\n",
    "ax.set_yticks(range(len(top_features_original)))\n",
    "ax.set_yticklabels(top_features_original.index, fontsize=11)\n",
    "ax.axvline(x=0, color='black', linewidth=1.5)\n",
    "ax.set_xlabel('Coeficiente (Log-Odds)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Top 10 Features m√°s Influyentes\\n(Verde = ‚Üë Riesgo | Rojo = ‚Üì Riesgo)', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "for bar, val in zip(bars, top_features_original.values):\n",
    "    width = bar.get_width()\n",
    "    ax.text(width + 0.02 if width > 0 else width - 0.02, bar.get_y() + bar.get_height()/2,\n",
    "            f'{val:.3f}', ha='left' if width > 0 else 'right', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70077ae4",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n de features:**\n",
    "- **Aumentan riesgo (verde):** pacientes j√≥venes (20-40 a√±os), m√°s visitas previas (`number_inpatient`), uso de medicaci√≥n para diabetes\n",
    "- **Disminuyen riesgo (rojo):** ciertos c√≥digos de pagador, pacientes muy mayores (90-100), algunos medicamentos espec√≠ficos\n",
    "\n",
    "Sorprendentemente, los pacientes m√°s j√≥venes presentan mayor riesgo de readmisi√≥n, posiblemente relacionado con menor adherencia al tratamiento o condiciones m√°s agudas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea3e3de",
   "metadata": {},
   "source": [
    "## √Årboles de decisi√≥n\n",
    "\n",
    "Los **√°rboles de decisi√≥n** son modelos no lineales que crean reglas de decisi√≥n interpretables. A diferencia de la regresi√≥n log√≠stica, pueden capturar relaciones m√°s complejas entre variables.\n",
    "\n",
    "### Visualizaci√≥n del √°rbol (profundidad limitada)\n",
    "\n",
    "Entrenamos un √°rbol con profundidad m√°xima de 3 niveles para poder visualizar e interpretar las reglas aprendidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaf6d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# √Årbol para visualizaci√≥n\n",
    "tree_viz = DecisionTreeClassifier(max_depth=3, class_weight='balanced', random_state=42)\n",
    "tree_viz.fit(X_train, y_train)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(22, 10))\n",
    "plot_tree(tree_viz, feature_names=X.columns, class_names=['No Readmitido', 'Readmitido'], \n",
    "          filled=True, rounded=True, fontsize=9, proportion=True, ax=ax, impurity=True, precision=2)\n",
    "ax.set_title('√Årbol de Decisi√≥n (Primeras 3 Capas)', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71017306",
   "metadata": {},
   "source": [
    "**Interpretaci√≥n del √°rbol:**\n",
    "- La primera divisi√≥n usa `number_inpatient` (visitas previas como paciente interno): son pacientes con m√°s visitas previas que tienen mayor probabilidad de readmisi√≥n\n",
    "- Las siguientes divisiones usan `discharge_disposition_id` (tipo de alta) y otros indicadores de uso previo del sistema de salud\n",
    "- Los nodos m√°s oscuros (azul intenso) indican mayor proporci√≥n de readmitidos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c292e8f6",
   "metadata": {},
   "source": [
    "### Diagn√≥stico de overfitting\n",
    "\n",
    "Comparamos el rendimiento en train vs test con un √°rbol sin restricciones para detectar sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b148de18",
   "metadata": {},
   "source": [
    "## Validaci√≥n cruzada y optimizaci√≥n\n",
    "\n",
    "Para asegurar que nuestros resultados son robustos y no dependen de una √∫nica divisi√≥n train/test, utilizamos **validaci√≥n cruzada**. Adem√°s, buscamos los mejores hiperpar√°metros para el √Årbol de Decisi√≥n mediante **GridSearchCV**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70752890",
   "metadata": {},
   "source": [
    "### Validaci√≥n cruzada (10-Fold) para la Regresi√≥n Log√≠stica\n",
    "\n",
    "Evaluamos el modelo en 10 particiones diferentes para obtener una estimaci√≥n m√°s confiable de su rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c55028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "import time\n",
    "\n",
    "cv_metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "cv_results_log = {}\n",
    "\n",
    "for metric in cv_metrics:\n",
    "    scores = cross_val_score(log_reg, X_train, y_train, cv=10, scoring=metric)\n",
    "    cv_results_log[metric] = {'mean': scores.mean(), 'std': scores.std()}\n",
    "    print(f\"{metric.capitalize():<12}: {scores.mean():.4f} ¬± {scores.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07f7ba9",
   "metadata": {},
   "source": [
    "Los resultados muestran consistencia entre los 10 folds (desviaciones est√°ndar bajas). El Recall de ~0.53 indica que detectamos aproximadamente la mitad de los pacientes readmitidos.\n",
    "\n",
    "### GridSearch para √Årbol de Decisi√≥n\n",
    "\n",
    "Buscamos la combinaci√≥n √≥ptima de hiperpar√°metros probando 40 combinaciones diferentes con validaci√≥n cruzada de 5 folds, optimizando para F1-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1586ca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [5, 8, 10, 12, 15],\n",
    "    'min_samples_leaf': [10, 20, 50, 100],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    DecisionTreeClassifier(class_weight='balanced', random_state=42), \n",
    "    param_grid, cv=5, scoring='f1', n_jobs=-1, verbose=0\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_tree = grid_search.best_estimator_\n",
    "\n",
    "print(f\"Mejores par√°metros encontrados:\")\n",
    "for param, value in grid_search.best_params_.items():\n",
    "    print(f\"  ‚Ä¢ {param}: {value}\")\n",
    "print(f\"\\nMejor F1-Score (CV): {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71d4b4e",
   "metadata": {},
   "source": [
    "El GridSearch encuentra que los mejores par√°metros son `max_depth=5`, `min_samples_leaf=50` y `criterion=entropy`. Esto confirma que un √°rbol m√°s podado (menos profundo) generaliza mejor que uno sin restricciones.\n",
    "\n",
    "### Comparativa Final de Modelos\n",
    "\n",
    "Evaluamos todos los modelos entrenados en el conjunto de test para obtener una comparaci√≥n justa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe47b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Regresi√≥n Log√≠stica': log_reg,\n",
    "    '√Årbol Simple (depth=3)': tree_viz,\n",
    "    '√Årbol Completo (sin poda)': tree_full,\n",
    "    '√Årbol Optimizado (GridSearch)': best_tree\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    fpr_temp, tpr_temp, _ = roc_curve(y_test, y_prob)\n",
    "    roc_auc_temp = auc(fpr_temp, tpr_temp)\n",
    "    \n",
    "    results.append({\n",
    "        'Modelo': name,\n",
    "        'Accuracy': report['accuracy'],\n",
    "        'Precision': report['1']['precision'],\n",
    "        'Recall': report['1']['recall'],\n",
    "        'F1-Score': report['1']['f1-score'],\n",
    "        'AUC-ROC': roc_auc_temp\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results).set_index('Modelo')\n",
    "df_results.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d990b808",
   "metadata": {},
   "source": [
    "**An√°lisis de resultados:**\n",
    "- El **√Årbol Optimizado** obtiene el mejor F1-Score (~0.27) y AUC-ROC (~0.66)\n",
    "- La **Regresi√≥n Log√≠stica** tiene un rendimiento muy similar, siendo m√°s interpretable\n",
    "- El **√Årbol Simple (depth=3)** maximiza el Recall (~0.76) pero con baja Precision\n",
    "- El **√Årbol Completo** sufre overfitting severo, con el peor rendimiento general\n",
    "\n",
    "### Visualizaci√≥n Comparativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126240b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "models_names = df_results.index.tolist()\n",
    "x_pos = np.arange(len(models_names))\n",
    "width = 0.2\n",
    "\n",
    "# Gr√°fica 1: Precision, Recall, F1\n",
    "ax1 = axes[0]\n",
    "ax1.bar(x_pos - width, df_results['Precision'], width, label='Precision', color=COLORS['primary'], edgecolor='white')\n",
    "ax1.bar(x_pos, df_results['Recall'], width, label='Recall', color=COLORS['danger'], edgecolor='white')\n",
    "ax1.bar(x_pos + width, df_results['F1-Score'], width, label='F1-Score', color=COLORS['success'], edgecolor='white')\n",
    "ax1.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('M√©tricas por Modelo (Clase: Readmitido)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels([m.replace(' ', '\\n') for m in models_names], fontsize=9)\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Gr√°fica 2: AUC-ROC\n",
    "ax2 = axes[1]\n",
    "bars = ax2.bar(x_pos, df_results['AUC-ROC'], color=[COLORS['primary'], COLORS['info'], COLORS['warning'], COLORS['success']], edgecolor='white')\n",
    "ax2.axhline(y=0.5, color='gray', linestyle='--', linewidth=2, label='Aleatorio')\n",
    "ax2.set_ylabel('AUC-ROC', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('√Årea Bajo la Curva ROC', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels([m.replace(' ', '\\n') for m in models_names], fontsize=9)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "ax2.set_ylim(0, 1)\n",
    "for bar in bars:\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.02, f'{bar.get_height():.3f}', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41123b7a",
   "metadata": {},
   "source": [
    "### Curvas ROC Comparativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaf1471",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9, 7))\n",
    "colors_list = [COLORS['primary'], COLORS['info'], COLORS['warning'], COLORS['success']]\n",
    "\n",
    "for (name, model), color in zip(models.items(), colors_list):\n",
    "    y_prob_temp = model.predict_proba(X_test)[:, 1]\n",
    "    fpr_temp, tpr_temp, _ = roc_curve(y_test, y_prob_temp)\n",
    "    roc_auc_temp = auc(fpr_temp, tpr_temp)\n",
    "    ax.plot(fpr_temp, tpr_temp, color=color, lw=2.5, label=f'{name} (AUC={roc_auc_temp:.3f})')\n",
    "\n",
    "ax.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', label='Aleatorio')\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlabel('Tasa de Falsos Positivos (FPR)', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Tasa de Verdaderos Positivos (TPR)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Curvas ROC Comparativas', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f9afeb",
   "metadata": {},
   "source": [
    "Las curvas ROC confirman que el **√Årbol Optimizado** y la **Regresi√≥n Log√≠stica** tienen el mejor poder discriminativo, mientras que el **√Årbol Completo** (sin poda) tiene el peor rendimiento debido al overfitting.\n",
    "\n",
    "### Recomendaci√≥n Final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f157adfe",
   "metadata": {},
   "source": [
    "En contexto m√©dico, el **recall** es cr√≠tico: preferimos detectar m√°s pacientes en riesgo (aunque tengamos falsos positivos) que dejar pasar pacientes que ser√°n readmitidos. Sin embargo, si los recursos del hospital son limitados, el **f1-Score** ofrece un mejor equilibrio.\n",
    "\n",
    "El **√°rbol optimizado** o la **Regresi√≥n Log√≠stica** son las mejores opciones, dependiendo de si priorizamos interpretabilidad (√°rboles) o simplicidad (regresi√≥n)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efec4f7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Referencias\n",
    "\n",
    "1. Abdul-Aziz AA, Hayward RA, Aaronson KD, Hummel SL. Association Between Medicare Hospital Readmission Penalties and 30-Day Combined Excess Readmission and Mortality. JAMA Cardiol. 2017;2(2):200‚Äì203. doi:10.1001/jamacardio.2016.3704\n",
    "2. Clore, J., Cios, K., DeShazo, J., & Strack, B. (2014). Diabetes 130-US Hospitals for Years 1999-2008 [Dataset]. UCI Machine Learning Repository. https://doi.org/10.24432/C5230J.\n",
    "3. Dunbar P, Hall M, Gay JC, Hoover C, Markham JL, Bettenhausen JL, Perrin JM, Kuhlthau KA, Crossman M, Garrity B, Berry JG. Hospital Readmission of Adolescents and Young Adults With Complex Chronic Disease. JAMA Netw Open. 2019 Jul 3;2(7):e197613. doi: 10.1001/jamanetworkopen.2019.7613. PMID: 31339547; PMCID: PMC6659144.\n",
    "4. Freire, A. X., Umpierrez, G. E., Afessa, B., Latif, K. A., Bridges, L., & Kitabchi, A. E. (2002). Predictors of intensive care unit and hospital length of stay in diabetic ketoacidosis. Journal of Critical Care, 17(4), 207‚Äì211. https://doi.org/10.1053/jcrc.2002.36755\n",
    "5. Jade Gek Sang Soh, Amartya Mukhopadhyay, Bhuvaneshwari Mohankumar, Swee Chye Quek, Bee Choo Tai, Predicting and Validating 30-day Hospital Readmission in Adults With Diabetes Whose Index Admission Is Diabetes-related, The Journal of Clinical Endocrinology & Metabolism, Volume 107, Issue 10, October 2022, Pages 2865‚Äì2873, https://doi.org/10.1210/clinem/dgac380\n",
    "6. Ministry of Health. (2017, 11 abril). Diagnostic Code Descriptions (ICD-9) - Province of British Columbia. https://www2.gov.bc.ca/gov/content/health/practitioner-professional-resources/msp/physicians/diagnostic-code-descriptions-icd-9\n",
    "7. Scikit-learn Documentation - https://scikit-learn.org/\n",
    "8. Strack, B., DeShazo, J. P., Gennings, C., et al. (2014). Impact of HbA1c Measurement on Hospital Readmission Rates: Analysis of 70,000 Clinical Database Patient Records. *BioMed Research International*.\n",
    "\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
