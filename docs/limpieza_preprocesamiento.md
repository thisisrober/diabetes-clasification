# Documentaci√≥n: Limpieza y Preprocesamiento de Datos

---

## üìã Tabla de Contenidos

1. [Resumen Ejecutivo](#resumen-ejecutivo)
2. [Inspecci√≥n Inicial](#inspecci√≥n-inicial)
3. [Limpieza de Datos](#limpieza-de-datos)
4. [Transformaci√≥n de Variables](#transformaci√≥n-de-variables)
5. [Feature Engineering](#feature-engineering)
6. [Normalizaci√≥n](#normalizaci√≥n)
7. [Resultado Final](#resultado-final)

---

## 1. Resumen Ejecutivo

Este documento describe el proceso completo de limpieza y preprocesamiento del dataset de pacientes diab√©ticos. El objetivo es preparar los datos para entrenar modelos de machine learning que predigan la readmisi√≥n hospitalaria.

### Dimensiones del Dataset
- **Original:** 101,766 registros √ó 50 columnas
- **Final:** 101,766 registros √ó ~60 columnas (despu√©s de encoding y feature engineering)

### Cambios Principales
- ‚úÖ Eliminaci√≥n de columna `weight` (97% de valores faltantes)
- ‚úÖ Imputaci√≥n de valores faltantes en `race` con la moda
- ‚úÖ Sustituci√≥n de valores `?` por `Unknown` en `payer_code` y `medical_specialty`
- ‚úÖ Creaci√≥n de variables objetivo binarias
- ‚úÖ Encoding de variables categ√≥ricas (Label + One-Hot)
- ‚úÖ Feature Engineering (3 nuevas caracter√≠sticas)
- ‚úÖ Normalizaci√≥n de variables num√©ricas con StandardScaler

---

## 2. Inspecci√≥n Inicial

### 2.1 Carga del Dataset

```python
import pandas as pd
import numpy as np

df = pd.read_csv("data/diabetic_data.csv")
```

**Resultado:**
- 101,766 registros
- 50 columnas
- Tipos de datos mixtos (num√©ricos y categ√≥ricos)

### 2.2 An√°lisis de Estructura

Se utilizaron las siguientes funciones para inspeccionar el dataset:

- **`.info()`**: Informaci√≥n sobre tipos de datos y valores no nulos
- **`.describe()`**: Estad√≠sticas descriptivas de variables num√©ricas
- **`.head()`**: Vista previa de las primeras filas
- **`.shape`**: Dimensiones del dataset

### 2.3 Detecci√≥n de Valores Faltantes

#### Valores representados como '?'
El dataset utiliza el car√°cter `?` para representar valores faltantes en variables categ√≥ricas:

| Columna | Valores Faltantes | Porcentaje |
|---------|-------------------|------------|
| `weight` | ~97,000 | 97% |
| `payer_code` | ~40,000 | 40% |
| `medical_specialty` | ~49,000 | 49% |
| `race` | ~2,000 | 2% |

**Decisi√≥n tomada:**
- `weight`: Eliminada (demasiados valores faltantes)
- `race`: Imputada con la moda (mayor√≠a de valores completos)
- `payer_code` y `medical_specialty`: Valores `?` reemplazados por `Unknown`

### 2.4 An√°lisis de Duplicados

```python
# Duplicados por encounter_id (cada encuentro debe ser √∫nico)
duplicate_encounters = df['encounter_id'].duplicated().sum()

# An√°lisis de pacientes √∫nicos
unique_patients = df['patient_nbr'].nunique()
```

**Hallazgos:**
- No hay duplicados en `encounter_id` ‚úì
- Pacientes √∫nicos: ~71,000
- Promedio de encuentros por paciente: ~1.43
- Algunos pacientes tienen m√∫ltiples readmisiones (dato esperado)

### 2.5 Operaciones Vectorizadas con NumPy

Se demostr√≥ la eficiencia de las operaciones vectorizadas compar√°ndolas con loops tradicionales:

#### Comparaci√≥n de Rendimiento

| Operaci√≥n | Loop Tradicional | NumPy Vectorizado | Speedup |
|-----------|------------------|-------------------|---------|
| Suma | ~10ms | ~0.5ms | 20x |
| Media | ~8ms | ~0.3ms | 26x |
| Normalizaci√≥n Min-Max | ~50ms | ~1ms | 50x |

**Conclusi√≥n:** Las operaciones vectorizadas de NumPy son significativamente m√°s r√°pidas y eficientes, especialmente con datasets grandes.

```python
# Ejemplo de vectorizaci√≥n
time_in_hospital = df['time_in_hospital'].values

# Vectorizado (r√°pido)
normalized = (time_in_hospital - np.min(time_in_hospital)) / (np.max(time_in_hospital) - np.min(time_in_hospital))

# vs Loop tradicional (lento)
min_val = min(time_in_hospital)
max_val = max(time_in_hospital)
normalized_loop = [(x - min_val) / (max_val - min_val) for x in time_in_hospital]
```

---

## 3. Limpieza de Datos

### 3.1 Eliminaci√≥n de Columnas con Mayor√≠a de Nulos

**Columna eliminada:** `weight`

**Justificaci√≥n:**
- 97% de valores faltantes
- Imputar tantos valores ser√≠a poco confiable
- No es una variable cr√≠tica para el modelo

```python
df = df.drop(columns=['weight'])
```

### 3.2 Imputaci√≥n de Valores Faltantes

#### 3.2.1 Imputaci√≥n de `race` con la Moda

**Estrategia:** SimpleImputer con estrategia `most_frequent`

```python
from sklearn.impute import SimpleImputer

# Convertir '?' a NaN
df['race'] = df['race'].replace('?', np.nan)

# Imputar con la moda
race_imputer = SimpleImputer(strategy='most_frequent')
df['race'] = race_imputer.fit_transform(df[['race']]).ravel()
```

**Resultado:**
- ~2,273 valores imputados
- Valor imputado: `Caucasian` (categor√≠a m√°s frecuente)

#### 3.2.2 Sustituci√≥n de '?' por 'Unknown'

Para `payer_code` y `medical_specialty`, se decidi√≥ mantener la informaci√≥n de que el valor es desconocido en lugar de imputar.

```python
df['payer_code'] = df['payer_code'].replace('?', 'Unknown')
df['medical_specialty'] = df['medical_specialty'].replace('?', 'Unknown')
```

**Justificaci√≥n:**
- Estas variables tienen muchos valores faltantes (40-49%)
- El hecho de que sean desconocidas puede ser informaci√≥n relevante
- Evitamos introducir sesgo mediante imputaci√≥n

---

## 4. Transformaci√≥n de Variables

### 4.1 Variable Objetivo: `readmitted`

La variable original `readmitted` tiene 3 categor√≠as:
- `NO`: No readmitido
- `<30`: Readmitido en menos de 30 d√≠as
- `>30`: Readmitido en m√°s de 30 d√≠as

Se crearon **dos variables binarias** para diferentes enfoques de modelado:

#### Opci√≥n 1: `readmitted_binary`
Clasificaci√≥n binaria simple: readmitido vs no readmitido

```python
df['readmitted_binary'] = (df['readmitted'] != 'NO').astype(int)
```

- **0:** No readmitido
- **1:** Readmitido (cualquier tiempo)

**Distribuci√≥n:**
- Clase 0: ~54%
- Clase 1: ~46%
- Desbalance moderado

#### Opci√≥n 2: `early_readmission` (RECOMENDADA)
Clasificaci√≥n enfocada en readmisiones cr√≠ticas tempranas

```python
df['early_readmission'] = (df['readmitted'] == '<30').astype(int)
```

- **0:** No readmitido o readmitido >30 d√≠as
- **1:** Readmitido <30 d√≠as (m√°s cr√≠tico)

**Distribuci√≥n:**
- Clase 0: ~89%
- Clase 1: ~11%
- Desbalance significativo (ratio ~8:1)

**‚ö†Ô∏è Importante:** Debido al desbalance, se recomienda usar:
- `class_weight='balanced'` en los modelos
- O aplicar SMOTE (Synthetic Minority Over-sampling Technique)

### 4.2 Encoding de Variables Categ√≥ricas

El dataset contiene m√∫ltiples variables categ√≥ricas que deben convertirse a formato num√©rico.

#### 4.2.1 Identificaci√≥n de Columnas Categ√≥ricas

Se identificaron ~24 columnas categ√≥ricas (tipo `object`), excluyendo:
- `encounter_id`, `patient_nbr` (identificadores)
- `readmitted` (ya procesada)

**Clasificaci√≥n por cardinalidad:**
- **Baja cardinalidad (‚â§10 valores √∫nicos):** 15 columnas
  - Ejemplos: `gender`, `age`, `change`, `diabetesMed`
- **Alta cardinalidad (>10 valores √∫nicos):** 9 columnas
  - Ejemplos: `admission_type_id`, `discharge_disposition_id`, `diag_1`, `diag_2`, `diag_3`

#### 4.2.2 Label Encoding

**Aplicado a:** Variables de baja cardinalidad

```python
from sklearn.preprocessing import LabelEncoder

label_encoders = {}
for col in low_cardinality_cols:
    le = LabelEncoder()
    df_encoded[col] = le.fit_transform(df[col].astype(str))
    label_encoders[col] = le
```

**Ventajas:**
- No aumenta la dimensionalidad
- Apropiado para variables con relaci√≥n ordinal impl√≠cita
- Eficiente en memoria

**Ejemplos:**
- `gender`: Female=0, Male=1
- `age`: [0-10)=0, [10-20)=1, ..., [90-100)=9
- `change`: No=0, Ch=1

#### 4.2.3 One-Hot Encoding

**Aplicado a:** Variables nominales importantes de alta cardinalidad

```python
# Variables seleccionadas para One-Hot Encoding
onehot_cols = ['admission_type_id', 'discharge_disposition_id', 'admission_source_id']

df_encoded = pd.get_dummies(df_encoded, columns=onehot_cols, 
                             prefix=onehot_cols, drop_first=True)
```

**Ventajas:**
- No asume relaci√≥n ordinal entre categor√≠as
- Cada categor√≠a se representa como una feature binaria independiente
- Apropiado para variables nominales

**Consideraciones:**
- Se us√≥ `drop_first=True` para evitar multicolinealidad
- Para variables con demasiadas categor√≠as (ej: c√≥digos de diagn√≥stico), se aplic√≥ Label Encoding para evitar explosi√≥n dimensional

#### 4.2.4 Variables de Medicaci√≥n

Variables como `metformin`, `insulin`, `glyburide`, etc., tienen valores:
- `No`: No se prescribi√≥
- `Steady`: Dosis constante
- `Up`: Dosis aumentada
- `Down`: Dosis reducida

**Tratamiento:** Label Encoding (orden impl√≠cito: No < Steady < Up/Down)

---

## 5. Feature Engineering

Se crearon **3 nuevas caracter√≠sticas** combinadas para capturar patrones relevantes:

### 5.1 `total_visits`

**Definici√≥n:** Suma total de visitas m√©dicas previas

```python
df_encoded['total_visits'] = (df_encoded['number_outpatient'] + 
                               df_encoded['number_emergency'] + 
                               df_encoded['number_inpatient'])
```

**Justificaci√≥n:**
- Un paciente con m√°s visitas previas puede tener mayor riesgo de readmisi√≥n
- Captura el historial de interacci√≥n con el sistema de salud

**Estad√≠sticas:**
- Rango: [0, 21]
- Media: ~0.68 visitas

### 5.2 `medication_changes`

**Definici√≥n:** Indicador de cambios en el tratamiento

```python
# Convertir a num√©rico
df_encoded['change'] = df_encoded['change'].map({'No': 0, 'Ch': 1})
df_encoded['diabetesMed'] = df_encoded['diabetesMed'].map({'No': 0, 'Yes': 1})

# Crear feature combinada
df_encoded['medication_changes'] = df_encoded['change'] + df_encoded['diabetesMed']
```

**Valores posibles:**
- **0:** Sin cambios y sin medicaci√≥n diab√©tica
- **1:** Un cambio (medicaci√≥n O cambio de dosis)
- **2:** Ambos (medicaci√≥n Y cambio de dosis)

**Justificaci√≥n:**
- Los cambios en medicaci√≥n pueden indicar condici√≥n inestable
- Puede correlacionar con mayor riesgo de readmisi√≥n

### 5.3 `procedures_per_day`

**Definici√≥n:** Intensidad de procedimientos durante la hospitalizaci√≥n

```python
df_encoded['procedures_per_day'] = df_encoded['num_procedures'] / (df_encoded['time_in_hospital'] + 1)
```

**Justificaci√≥n:**
- Un paciente con m√°s procedimientos por d√≠a puede tener condici√≥n m√°s severa
- Normaliza el n√∫mero de procedimientos por la duraci√≥n de la estancia

**Notas:**
- Se suma 1 al denominador para evitar divisi√≥n por cero
- Media: ~0.18 procedimientos por d√≠a

---

## 6. Normalizaci√≥n

### 6.1 StandardScaler para Variables Num√©ricas

**Objetivo:** Estandarizar variables num√©ricas a media=0 y desviaci√≥n est√°ndar=1

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

numerical_cols = ['time_in_hospital', 'num_lab_procedures', 'num_procedures', 
                  'num_medications', 'number_outpatient', 'number_emergency', 
                  'number_inpatient', 'number_diagnoses', 'total_visits', 
                  'medication_changes', 'procedures_per_day']

df_encoded[numerical_cols] = scaler.fit_transform(df_encoded[numerical_cols])
```

**¬øPor qu√© es importante?**
- Muchos algoritmos (SVM, KNN, Redes Neuronales) son sensibles a la escala
- Variables con rangos grandes pueden dominar el aprendizaje
- Mejora la convergencia de algoritmos de optimizaci√≥n

**Validaci√≥n:**
- Media despu√©s de escalar: ~0.0 ‚úì
- Desviaci√≥n est√°ndar: ~1.0 ‚úì

**Columnas normalizadas:**
- Variables num√©ricas originales (8)
- Features de engineering (3)

---

## 7. Resultado Final

### 7.1 Dataset Limpio Guardado

```python
df_encoded.to_csv("data/diabetes_clean.csv", index=False)
```

**Ubicaci√≥n:** `data/diabetes_clean.csv`

### 7.2 Dimensiones Finales

- **Registros:** 101,766 (sin p√©rdida de datos)
- **Columnas:** ~60 (despu√©s de encoding y feature engineering)
- **Variables objetivo:** 2 (`readmitted_binary`, `early_readmission`)

### 7.3 Estructura del Dataset Limpio

#### Columnas Eliminadas
- `weight` (97% faltantes)
- `readmitted` (reemplazada por variables binarias)
- `race_backup` (columna temporal)

#### Columnas Nuevas
- **Variables objetivo:**
  - `readmitted_binary`
  - `early_readmission`
- **Feature Engineering:**
  - `total_visits`
  - `medication_changes`
  - `procedures_per_day`
- **One-Hot Encoding:**
  - `admission_type_id_*` (m√∫ltiples columnas binarias)
  - `discharge_disposition_id_*`
  - `admission_source_id_*`

### 7.4 Tipos de Datos Finales

| Tipo | Cantidad |
|------|----------|
| `int64` | ~40 columnas |
| `float64` | ~20 columnas |
| `object` | 0 columnas (todas convertidas) |

### 7.5 Distribuci√≥n de Variables Objetivo

#### `early_readmission` (RECOMENDADA)
```
0 (No readmitido/<30 d√≠as):  ~90,600 (89%)
1 (Readmitido <30 d√≠as):     ~11,166 (11%)
```

**Ratio de desbalance:** 8.1:1

**Recomendaciones para el modelado:**
1. Usar `class_weight='balanced'` en modelos que lo soporten
2. Considerar SMOTE para sobremuestreo sint√©tico
3. Evaluar con m√©tricas apropiadas: F1-Score, Precision, Recall, AUC-ROC
4. No confiar √∫nicamente en Accuracy debido al desbalance

---

## 8. Checklist de Verificaci√≥n

- [x] **Carga e inspecci√≥n inicial**
  - [x] `.info()`, `.describe()`, `.head()`, `.shape`
  - [x] Detecci√≥n de valores faltantes
  - [x] An√°lisis de duplicados
  - [x] Demostraci√≥n de operaciones vectorizadas con NumPy

- [x] **Limpieza de datos**
  - [x] Eliminaci√≥n de columna `weight`
  - [x] Imputaci√≥n de `race` con moda
  - [x] Sustituci√≥n de '?' por 'Unknown' en `payer_code` y `medical_specialty`

- [x] **Transformaci√≥n de variables**
  - [x] Creaci√≥n de variable objetivo binaria `readmitted_binary`
  - [x] Creaci√≥n de variable objetivo `early_readmission`
  - [x] An√°lisis de desbalance de clases
  - [x] Label Encoding para variables de baja cardinalidad
  - [x] One-Hot Encoding para variables nominales cr√≠ticas
  - [x] Encoding de variables de medicaci√≥n

- [x] **Feature Engineering**
  - [x] `total_visits`: Suma de visitas previas
  - [x] `medication_changes`: Cambios en tratamiento
  - [x] `procedures_per_day`: Intensidad de procedimientos

- [x] **Normalizaci√≥n**
  - [x] StandardScaler aplicado a variables num√©ricas
  - [x] Validaci√≥n de media ‚âà 0 y std ‚âà 1

- [x] **Guardado y documentaci√≥n**
  - [x] Dataset limpio guardado en `data/diabetes_clean.csv`
  - [x] Documentaci√≥n completa del proceso
  - [x] Preservaci√≥n de 101,766 registros
